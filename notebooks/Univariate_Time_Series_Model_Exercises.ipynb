{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\section{Exercises (starred items have $R$-codes available)}\n",
    "\n",
    "\n",
    "\\begin{enumerate}[1.]\n",
    "\\item Consider the exchange rate daily data from December 4, 2006 to November 5, 2010 (Rupee versus Dollar, Pound and Euro), Rates.csv.\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item Compute the sample average, standard deviation and the first order autocorrelation of daily returns over the entire sample period.  Test if the mean and the first order autocorrelation are significantly different from zero using the tests proposed in Section 2.3.2. \n",
    "\n",
    "\\item Plot histograms of the returns over the entire sample period; Does the distribution look normal?  Test it through Jarque-Bera test in (\\ref{eqn:2JB}).\n",
    "\n",
    "\\item Aggregate the data at the weekly level; do (a) and (b) on the aggregated data. Compare the result with the result for the daily level.\n",
    "\\end{enumerate}\n",
    "\n",
    "\\item For the returns of all three time series in problem 1,  construct\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item ARMA models for the returns; identify the model structure via ACF and PACF.\n",
    "\n",
    "\\item  GARCH models for the squared returns; compare the model coefficients for the three series.  Comment on the difference if any.\n",
    "\n",
    "\\item Is there a co-movement among the three exchange-rate series? To make the plots on a comparable scale, convert the starting points of the series unity. Does the co-movement vary over different time regimes? (Back up your claim with solid analysis.) Identify the transition states and speculate how you can exploit this for trading decisions. \n",
    "\\end{enumerate}\n",
    "\n",
    "\\item Exchange Rates: The file \\texttt{Exchange Rates.csv} contains exchange rates between US dollar and twenty-five major currencies. The daily data spans from Jan 3, 2000 to April 10, 2012. Use this data to perform the following tasks: after computing the returns, $r_t=\\log(p_t)-\\log(p_{t-1})$ for each series.\n",
    "\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item Plot histograms of the returns and test if the distributions are normal, via Q--Q plots.\n",
    "\n",
    "\\item Compute the auto-correlation function and identify which lags are significant. What is the tentative ARMA model?\n",
    "\n",
    "\\item Is there any ARCH effect? Why?\n",
    "\n",
    "\\item Fit GARCH(1,1,) and IGARCH(1,1) models using both normal and t-distributions for the innovations. Which volatility model appears to be the best?\n",
    "\\end{enumerate}\n",
    "\n",
    " \\item Exchange rates: Consider the original series, $p_t$, for the duration starting June 11, 2003.\n",
    " Answer the following after dividing each series by $p_1$, so that the starting points are the same.\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item Identify for each series, if there are any regime changes. Test if these changes are statistically significant.\n",
    "\n",
    "\\item Identify how many clusters are there and if the membership changes under different regimes.\n",
    "\n",
    "\\item Do the series move together? Provide some intuitive tests (more formal tests will be covered in Chapter 3). \n",
    "\n",
    "\\item Compare the results in (c)  with (b). What is the connection between clustering and co-movement? Briefly discuss.\n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "\\item Consider daily price of Apple stock from January 2, 2015 to January 2, 2018. The data can be obtained from Yahoo Finance and have 7 columns (namely, Date, Open, High, Low, Close, Adj Close, Volume). We focus on the adjusted closing price in the last column.\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item Compute the daily log returns. Is there any serial correlation in the daily log returns? Use the test for white noise as outlined in the text. \n",
    "\n",
    "\\item Consider the pivot based on the pivot based on the average of high and low price and the pivot based on the average of high, low and close prices. Compute the returns based on the pivot log prices and test for serial correlation. Compare this result with the finding in (a). \n",
    "\n",
    "\\item Consider the log price series of AAPL stock. Is the log price series unit-root nonstationary? Perform a unit-root (Dickey-Fuller) test to answer the question and present your conclusion.\n",
    "\\end{enumerate}\n",
    "\n",
    "\\item Consider daily price of Apple stock again.\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item Compute various measures of variance computed from the entries of the price bars. Comment on their correlation with log volume. \n",
    "\n",
    "\\item Use the ARIMA modeling to come up with a parsimonions model for log volume. Comment on the model accuracy by setting aside a validation data set. \n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "\\item The \\textit{Roll} model for trade prices discussed in the text can be more specifically stated as,\n",
    "\t\\[\n",
    "\t\\begin{split}\n",
    "\tp_t^*&= p_{t-1} + \\epsilon_t, \\;\\;\\;\\;\\; \\epsilon_t \\sim N(0,\\sigma^2) \\\\\n",
    "\t\\text{and } p_t&= p_t^* + c s_t, \\;\\;\\;\\;\\; \\text{where }s_t \\in \\{+1,-1\\}\n",
    "\t\\end{split}\n",
    "\t\\]\n",
    "Here $p_t^*$ is the ``true'' value of a security and $p_t$ is the observed trade price, which differ because of the bid-offer spread: $c$ is the half-spread and $s_t$ is the direction of the $t$th trade. $s_t$ and $\\epsilon$ are serially independent and independent of each other.\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item Calculate the serial correlation of the observed prices $p_t$. Construct an MA(1) model with the same autocorrelation structure. Of the two such models, take the invertible one. \n",
    "\n",
    "\\item For the invertible model, construct the associated AR model. \n",
    "\n",
    "\\item Use the model to show how to estimate the bid-ask spread and apply the results on the Apple data used in Problem (5) and (6). \\\\\n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "\n",
    "\\item Consider the level III data for the ticker, ``BIOF'' for a single day. The data description follows the similar details as given in Table~\\ref{tab:CISCO}. This exercises consists of two parts; in the first part, we want to construct the limit order book and the second part we want to study the effect of time aggregation. \n",
    "\n",
    "\\begin{enumerate}[(a)]\n",
    "\\item At any point in time, a limit-order book contains a number of buy and sell orders.\n",
    "\t\\begin{enumerate}[(i)]\n",
    "\t\\item Develop a program that can visually present a limit order book. Present the order book diagram as of 11:00~A.M. for the first tern positions on both sides.\n",
    "\t\\item Aggregate demand and supply are represented as step functions of shares accumulated at each price level. The height of step `$i$' on the demand side is the price side is the price difference between $(i-1)$th and $i$th best price. The height of the first step is the mid price; the length of a step, `$i$', on the demand side is the total number of shares across all orders until $i$th price. Similarly, the supply side can be constructed. Normalize a step height by the cumulative length from the first step to tenth step. Present the aggregate curve as of 11:00~A.M.. \n",
    "\t\\item Using the aggregate graph give in (ii), suggest measures for the order book imbalance---which is a key determinant in some entry/exit algorithms in the high-frequency setting.\n",
    "\t\\end{enumerate}\n",
    "\n",
    "\\item Aggregate the data to a 2-minute interval, summarizing the number of transactions, volume weighted average price and average duration between transactions. Also, produce the price bars (open, close, low and high and volume) for each interval.\n",
    "\t\\begin{enumerate}[(i)]\n",
    "\t\\item Briefly discuss how you would quantify the loss of information due to aggregation.\n",
    "\t\\item Calculate the variance of the returns based on the aggregated VWAP and compare it with the variance based on all observations.\n",
    "\t\\item From Exercise~7, it follows that the true price is measured with error, which is due to market friction. The aggregation done above may cancel out the friction and may reflect the true price. Alternatively, one can sample the price every two minutes and compute the variance in the returns. How does this compare to VWAP based descriptives?\n",
    "\t\\end{enumerate}\n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "\n",
    "\\end{enumerate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
