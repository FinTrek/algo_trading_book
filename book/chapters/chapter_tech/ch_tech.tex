% !TEX root = ../../book.tex
%\hfill
%\par\vspace{\baselineskip}

\chapter{The Technology Stack}\label{chap:ch_tech}
There are a broad spectrum of trading infrastructures and operations but none, we think, is as broad and complex as the one in a large electronic trading execution operation in a large Broker-Dealer. So we use that as a template for our exploration of the technology stack that is necessary to support such an operation. We st  art by reviewing the end to end flow of information and then go into some detail on the various components. As previously discussed all large ET businesses also operate their own ATS so we'll briefly look at the technology setup needed to support that use case as well.\\

\section{From client Instruction to Trade Reconciliation}
TODO Build an end-to-end diagram

Figure~\ref{fig:pexchrate} shows the end to end diagram of a hypothetical trading infrastructure. We can use this diagram to follow the full lifecycle of a client order and the main components of the infrastructure involved.
\subsection{Client Side}
The trader enters an algorithmic order into her Execution Management System (EMS) a specialized trading infrastructure that integrates with internal systems and provides all the tools an institutional traders needed to manage her day to day trading needs. EMS are most often vendor products with Ez Castle \footnote{\url{https://www.ezesoft.com/}}, Portware \footnote{\url{http://www.portware.com/}}and FlexTrade \footnote{\url{https://flextrade.com/}} some of the most popular options. Many larger buy-side trade organizations have more complex needs and often have an homegrown solution.\\

The trader chooses from a drop-down menu the particular provider and Algo strategy and  chooses the desired parameters. Every broker that wants to expose their execution into an EMS needs to certify with the EMS by providing a FIX Specification Document that highlights what strategy are available and what parameters are available for each and the validation information about the parameter. The EMS integrates this into their system and exposes them in the front end for the trader to chose. FIX stands for \emph{Financial Information eXchange} and it's a standard communication protocol specifically designed for financial applications \footnote{\url{https://www.fixtrading.org/}}\\

Once the order is submitted the EMS uses a preconfigured network connection to transfer the order to the broker. This session is created as part of the \emph{Client Onboarding} step a quite laborious process to setup a new client relationship, configure the client financial limits and controls.

\subsection{Inbound Gateway}
We are now broker-side where the order is received by and Inbound gateway. The first think to happen is some form of validation to ensure that the message is a valid order and has all the necessary fields. Next step is to perform a set of risk and credit checks to ensure that the order is within the specified risk limits and the client overall exposure, the maximum notional the client is allowed to trade and have in the market at any point in time. Any mismatch in these validation steps and the order is rejected back to the client.

\subsection{Order Management System and Order Enrichment}
Once the order is validated it is created within the broker dealer infrastructure. Order life-cycle is quite complicated and nuanced and it is critical that the state of the order is always up to date and state transitions carefully managed. The role of creating and managing the state of the order is fulfilled by an Order Management System (OMS). These infrastructures are at the heart of any trading operation and are one of the most important components. They are often built around a Bus based architecture a software paradigm where loosely couple components communicated with each other over a messaging middleware. The advantage of this approach is that other ancillary components within the infrastructure can ``Listen in" on the messages and use the information to affect other systems or collect the data for trade reporting, and analysis.\\

One step often performed in this layer is Order Enrichment. This is a step that adds additional, lower level parameters that adjust and customize the execution behavior for the specific client/algorithm/parameters triplet. This step also translates nuances between the instruction the client sends and what the execution system understands. This is often accomplished by a specialized \emph{Rules Engine} a software library that allows a set of rules to be applied to an order and then re-evaluate the rules after any modification until no rules are triggered.

\subsection{Execution Strategy Stack}
And this is, as the famous expression quote: "Is where the magic happens!" The order reaches the software component where the Algo is actually executed. We'll delve into more details in on the execution stack in the next section so here we just assume that the strategy is initialized and starts executing. The strategy is associated with the order contained in the OMS often called the ``Parent Order" and as consequence of the strategy logic one or more ``child" orders  are created in the OMS and the sent forward for submission to one or more trading venue. Before the order are actually forwarded the order passes through an additional control layer to ensure the strategy does not violate the risk limits and speed bumps, general term used for limits that prevents the strategy from trading too fast or too aggressively or send too many child orders, etc.

\subsection{Outbound Gateway}
The child orders are received by another piece of infrastructure that is responsible for actually sending the orders to the market: The Outbound Gateway. All venues support one more protocols to communicate with market participants. Essentially all of them support a FIX protocol but in most cases they also support a much faster ``native" protocol that encodes the instructions in a compressed binary protocol. The role of the Outbound Gateway is to connect to the various venues and then act as a translation layer from the internal representation in the OMS to the external representation of the specific protocol implemented by the venue. The outbound gateway also listens to the connection callback to capture any asynchronous event coming from the venue like order insert/cancellation acknowledgements and executions events, and updates the state of the child order representation in the OMS.

\subsection{Notifications to the Client}
As the strategy executes orders in the market the OMS keeps the state of the parent order up-to-date and either every execution or periodically send updates back via the inbound gateway back to the client's EMS that update its own state and provide feedback to the trader that the strategy is executing,  what the average prices achieved, and other analytics necessary for the trader to understand how well the strategy is executing. The inbound control layer is also kept uptodate so that the total state of all client orders is accounted for if/when a new order from the same client is received.\\

\subsection{Middle and Back Office}
We are almost done. The step above completes the real-time feedback loop from the client through the executing strategy to the market and back. The rest of the processing is in most cases done offline by a set of infrastructures commonly referred as ``Middle and Back Office Systems". These systems play a critical role in the business and regulatory side of trading they are arguably the most important pieces of the puzzle without with no trading operation could function. As we previously discussed, performance of an algorithm is really just the cherry on the cake of trading. Without a traded knowing what she has traded and what her current position is, without the confidence that a once a buy order is executed, within the T+2 settlement period (more on this below) the shares she just bought will be in her account, nothing else would matter. Describing in detail the complexity and subtleties of the process that ensures that once an order is executed the shares or the monies are in the right account at the right time is well beyond the scope of this book. But the authors feel it's important that the reader has at least a vague understanding and a high degree of respect of the critical role these systems and the hard working people operating them and handling the myriad of exceptions and trade breaks are playing in our industry.\\

To start let's quickly review the process by which share ownership is transferred between large institutional sellers and the buyers. Stock ownership is almost never in paper form but very likely stored as a book entry in a computer at a place called DTC or Depository Trust Company. This firm act as the main custodian for all the shares that can freely trade on exchange. Brokers will have an account at the DTC who will keep custody of all the shares the broker holds on behalf of clients so from the DTC perspective the broker is the owner of the shares. The process of transferring ownership goes through 2 steps:
\begin{itemize}
\item Clearing: Is the process of updating the accounts of the trading parties and arranging for the transfer of money and securities. In most cases this step goes through yet another intermediary: a Central Clearing entity that provides various services in particular all trade aggregation so only the net shares across all trades are  transferred.
\item Settlement: Is the process of actual exchange of securities for cash. This in most cases (there are always exceptions in a complicated process) happen on T+2
\end{itemize}
The DTC then also handles a lot of additional functions like the handling on stock dividends, etc.\\

Investor cannot trade directly with each other but can only trade through an intermediary, a Broker. Large investor often trade with multiple broker but engages one (and some time more than one) as a Prime Broker where they centralize their balances. Prime brokers provide a ll set of services (e.g. stock lending, financing ) but we won't go in any more details here. The client will setup one or more often many accounts with the prime broker (sometimes thousands depending on the complexity of the operations) where they record what shares registered in DTC are actually belong to the client. \\

Let's finally turn to the topic at hand. In a executing broker middle and back office operation are segregated in two distinct parts managed independently: 
\begin{itemize}
\item The Market Side: Deal with the interaction with the various exchanges  and the clearing house and ensure that there is a perfect match between what the broker thinks it has traded and what actually was traded, at what price and in what way (i.e. taking vs. providing). This is then used to actually determine who much the broker needs to pay the various exchanges for their execution fees (or get paid by them if they earned a rebate)
\item The Client Side: Handles the clearing and allocation of all the traded shares to the various accounts based on the client instructions (mostly in electronic format but sometimes via email) and the ensure that their record match exactly what the client has and what their prime broker has in their records and handle the reconciliation of frequent breaks.
\end{itemize}

Each side is usually separated in a set of Middle office functions that handles the booking and allocations and the confirms with the client/exchanges and a back office that manages things around settlement.
The back office also handle the regulatory burden of  managing trade reporting to the TRF/ACT and the OATS process that sends to FINRA the full life-cycle of the order. As the reader will now appreciate the role of this function is absolutely critical to the correct function of a trading operation. The middle and back-office teams there are the truly unsung heroes of this book. Without them what we do would not matter one single bit.

\section{Algorithmic Trading Infrastructure}
TODO Add a Diagram with the main components

This section details a somewhat generic approach for algorithmic trading infrastructure. Different providers have different approaches to this. For instance some platform the core strategy is in one infrastructure but the order routing part is a separate (albeit similar) infrastructure. Other combine the two parts together, other still separate scheduling and order placement/routing components. The below simplified setup assumes an all-in-one approach.\\

The core of an Algorithmic Trader infrastructure is a software framework generically called ``Strategy Container". This facility sits in-between the low level OMS related functionality and the strategy and provides a set of abstractions and interfaces to simplify the development of a trading algorithm.  When a new order event is received from the OMS the strategy container reads and validates the instructions like the name of the strategy and the necessary and additional parameters. If this is a valid new order instruction it usually has to pass through and additional layer of controls to ensure that various variables such as order quantity or limit price are within the required boundaries. Once this validation step is completed the strategy container instantiates the code that encodes the particular strategy, initializes it with the specific parameters. It will also connect the algorithm to the necessary services the algorithms needs to implement the strategy.\\  Let's look at some of the main services and related infrastructures:
\subsection{Static Data Services}
An Algorithmic strategy requires a slew of reference data and other static data to operate. First and foremost the particular instrument the strategy is trading but also other information like primary exchange, exchange open/close time, etc. It will also require the normalizing analytics as well as any calibration parameters for the various models. These static data services abstract the sources of the underlying data that could be databases, flat files and calls to other systems.
\subsection{Market Data Facility}
Access to real-time market data is the most important component of any trading algorithm. Strategy container is usually provided with two way of accessing this data: via a data cache that is continuously kept uptodate by some underlying thread, and via callback. The data cache also continuously updates additional core analytics used by the strategy such as total traded volume (filtered for specific condition codes) and others. Natural question is, how do we actually get real-time market data? Connected to our market data facility is one of the most demanding and expensive pieces of infrastructure in our whole stack: the Market Data Plant. This is worth a small detour.
\subsubsection{Market Data Plant} 
Any serious trading operation, in particular in the post NMS era and the Order Protection Rule requires to access protected quotes, needs to subscribe and deliver direct market feeds from all protected exchanges. For most usecases top of book (Level I) data is not going to be sufficient and you need to subscribe to at least Level 2 or even better Level 3 data. Each exchange family (and sometime even within one) has it's own proprietary multicast protocol. Also very important information like trade condition codes are also proprietary and need to be normalized. For each level 3 market data feed on needs a book building library that interprets all the multicast event and updates the internal state of the book. Doing this in a very efficient way that can withstand market data spikes without dropping any multicast packet is not an easy task and requires skills and excellent networking and compute infrastructure something that is not cheap to build and maintain. More an more often trading operation rely on third party vendor that often provide a hybrid software and hardware solution to handle the whole operation. Some of the most important vendors in this space are: Reuters \footnote{\url{https://financial.thomsonreuters.com/en.html}}, Redline  Trading Solutions \footnote{\url{https://www.redlinetrading.com/}}, Exegy \footnote{\url{https://www.exegy.com/}}.
On top of the significant costs of operations  exchanges have ratcheted up the cost of market data subscription to a level some people could call extortionary making market data a significant component of the overall cost of running a trading operation (in the the six digit range!).
\subsubsection{Outbound Order Interface}
Back to our core services. The last core set of abstractions has to do with managing outbound child orders. This facility provides the strategy with the state of all outstanding child orders and ability to create, cancel and amend them in an asynchronous way and managing any exceptions like for example trying to cancel an order that has just executed on exchange but the fill event has not yet reached the OMS. 

\subsection{Main Strategy Loop}

TODO Add a diagram of the main loop, with state cache, etc?

We come to the \emph{core of the Core} of the whole infrastructure. While there is many ways to write a trading strategy a pattern has emerged that has become somewhat of a standard approach. It's sometimes referred as the Main Strategy Loop. \footnote{Some practitioners for example believe that the right conceptual framework for a strategy is that of a State Machine: Based on what event just happen there is a ``state transition" in the next state and writing a strategy is essentially to model and implement these state transitions. Conceptually elegant, in practice this approach is extremely hard to pull off as there are a lot of side effects and co-dependencies that makes it hard to cleanly decompose}
The strategy container other working threads ensure the state is always kept up to date. The strategy flow is encoded in essentially one stateless function that is called on either a timer, any major event such as quote change/trade/execution, etc., or just a tight loop. This function perform three phases:
\begin{enumerate}
\item Understand where the strategy is at any point in time by recovering the state from the state cache with all the information it needs. How many shares I have done and how many, based on my target strategy should I be at this point and where should I try to be in the near future. 
\item Based on the full information set and the current state of the outstanding child orders in the market determine which orders to cancel, which to amend, and which to submit.
\item Send these instructions to the Outbound Order Interface. In some cases step 2 just calculates the correct exposure it wants in the market and it's the role of this step to optimally decide if and how this should be implemented by either submitting additional orders or amending the existing ones, etc.
\end{enumerate}
After performing these three steps the main loop exist and repeats some time later. Since the approach is, in mathematical jargon, Markovian, meaning that i only depends on the existing state, if the strategy is called again immediately after it will most likely make the same decisions it just made and determine in step 2 that no action is necessary and the main loop simply exits.

\subsection{Additional Infrastructures}
depending on the complexity and sophistication of the operation the technology stack is often supplemented by additional components that serve a specific role. Let's look at a few of the most common.
\subsubsection{Realtime Analytics Engine}
More and more, algorithmic trading product have become highly sophisticated and dynamic, adapting behavior as market conditions change. Many of the normalization variables and signals are adjusted every single tick or a few trades to provide the most uptodate picture of the immediate and future market state. These analytics could be built within the current Algo engine but that is not only more cumbersome but also inefficient. In almost every single case one process cannot handle the compute load to run thousands of algorithms on all symbols and the infrastructure is striped in multiple processes handling a subset of symbols. Per symbol analytics could be still be built within each process but many analytics, like for example computing the real-time price of an index midpoint, it requires that hundreds of symbols need to be computed in the same process for each symbols creating large duplication and reducing the effectiveness of striping. Additionally the more analytics one creates the more the load on the process that should be used to handle the algorithmic decisions\\

For these and other reason of convenience calculating real-time analtyics if often delegated to a purpose-built infrastructure that provides ease of building additional analytics and combining existing ones in more and more complex signals. 

\subsubsection{Algorithm Switching Engine}
Most execution strategies have limited ability to change behavior when particular situations arise and opinionated traders might have strong beliefs that in certain situations a different trading approach might be warranted. Historically that trader might work with her favorite broker to build a customized strategy but that usually took a long time and then the trader could only use this approach with one broker. To provide simple dynamic customization approaches, broker started offering Algo of Algos functionality around what is commonly called an Algo Switching Engine. This Engine would look like a combination of a simple strategy container and a limited real-time analytics engine and where a an Algo is represented by a simple set of if-then-else rules. The condition  part of these rules will be simple combinations of the available signal and the action part will define an particular algo with associated parameters. \\

Once the order is received by the switching engine the rules are evaluated and an order for the specified strategy is sent to the strategy container. The rules are then re-evaluated every few seconds and if the conditions change the switching engine will send a cancel/replace message to the strategy container to change the underlying strategy. \\

With this simple approach on can build very sophisticated dynamic algorithms thus offering almost unlimited capabilities and have made algo switching engine very popular both with traders and with execution consultants that often differentiate themselves by smart usage of these approaches. \\
While Algo Switching Engine are a very powerful tool it has several meaningful downsides that one should consider.
\begin{itemize}
\item The rules that make up a custom strategy encode particular beliefs on the value of the particular triggering condition. If that condition had true predictive power that would imply that it should be considered an alpha signal that would be better used in a more nuanced way within the strategy rather than as a blunt instrument of an algo switching instruction. If the condition is actually, as most often the case, not predictive then the realized performance of the strategy will be, in general, hurt by the customization. Thus while in principle it's a good idea in practice it should be discouraged unless one can measure that there is real value in the approach and then one should incorporate it in the main algos. This would make algo switching engine ideal as a temporary exploratory tool rather that the overused tool it currently is.
\item  Algorithms are usually very poor are at persisting state across a cancel/replace transition. This usually implies that all state of the algorithm is lost and a completely new order is created. Algos are also relatively poor at the beginning of the order as various analytics and execution bands have poor behavior when initialized (e.g. participation bands unless handled correctly have a discontinuity at 0). Heavy use of switching transitions will have severe negative impact on algo performance.
\item Once a trader is onboarded into a custom strategy she practically loses any support of Post Trade support by the broker since the optionality embedded in the switching to a large extent relieve the broker from any performance obligation as the performance will be largely driven by the switching rules. Additionally these orders cannot be used b y brokers to measure and improve their own performance since the individual slices will be hopelessly biased by the switching rules
\end{itemize}

As one can glean from the above treatment of algo switching engine, the authors are not big fans of this approach and consider that this evolution has likely decreased the quality of execution algorithms. The popularity of these approaches is still quite high and we believe the approach is overused. However there are signs that this popularity is slowly waning as clients are starting to trust more the broker to be able to fine tune the behavior of the strategies.

\subsubsection{Portfolio Algorithm Engine}
We discussed in chapter \ref{chap:ch_exec_models} that some algorithmic strategies involved the coordinated trading of a whole portfolio of instruments. In this section we briefly looks at the technology infrastructure needed for this type of strategies and some of the common issues facing these problems.\\

The problems starts immediately at the interaction through the FIX infrastructure with the client's EMS. Until recently the support for Basket trading was  never well nor consistently supported by the various EMS making the certification of a portfolio strategy a potentially very fiddly proposition as multiple approaches might need to be supported. Where basket trading is not supported firms have resorted to simple tricks like considering a portfolio every order sent to that algo within a certain timeframe, or linked to a portfolio ID parameters. \textnote{Need to confirm that this is still the case} \\

The portfolio algorithms infrastructure is in not dissimilar to the one used in Algo Switching but with the added complication that it now needs to manage the schedule for multiple orders. This algorithms will create a set of linked trajectories, often through some form of portfolio optimization process that implements the specific objective function. \\

Once the trajectories have been generated the algorithm will, in most cases, use one or more existing  algos possibly modified to better work with the portfolio infrastructure.  Certain algo implementations might send short time slices to a TWAP algo ( essentially transforming the trajectory into a piece-wise linear curve ) while other implementation might use a modified VWAP style algo that supports a parameter that specifies the trajectory to be used. \\

Once the portfolio starts trading market condition changes or realized deviations of the portfolio shape (usually as an impact security measure the single slice will have a max POV constraints applied which if triggered will distort the original intent) usually require the algorithm to re-optimize and the individual slices are amended to account for the new schedule.\\

This workflow is already quite complex but there are several further mechanical and numerical issues that further complicates matter making portfolio algorithms really hard to pull off successfully. Here are several pitfalls that happen in practice:
\begin{itemize}
\item Portfolio IS style algorithms, the most common approach for portfolio algo is usually implemented by a QP (Quadratic Programming) \footnote{\url{https://en.wikipedia.org/wiki/Quadratic_programming}} optimization problem where the schedule is dicretized into bins representing the shares to be traded in a certain period. For large portfolios common in fund rebalancing and for relatively fine grained time bins  the number of variables that need to be used explodes pretty quickly (in particular formulating these problems require use of several auxiliary variables to manage cost constraints etc). For example a 1000 stock portfolio with 5m bins  sent to a whole day Portfolio IS algorithm would have to solve a problem with > 100k variables. This often results in a very slow  optimization cycle that can take 100s of seconds making it too slow  for effective utilization ( no client would want to wait several minutes to see the first trades) and too compute intensive for a scalable operation (e.g. one receives a dozen such portfolios at the beginning of the day). To solve this problem effectively requires non trivial numerical tricks and simplifications that makes the actual optimization problem very tricky to solve.
\item Essentially all such algorithms are discretized time bins and synchronizing the re-optimization to correctly line up is very tricky and often leads to purely mechanical distortion of the realized schedule even in very simple settings (e.g. the tendency of the orders to be behind schedule leads to the unexecuted quantity to be reallocated to the renaming bins causing a significantly stretched execution schedule).
\item The portfolio level optimization problem requires a variance-covariance matrix for all instruments in the portfolio. As discussed the universe of security that are traded with algos is usually very large and to have stable Var/Cov matrix for such universe requires the creation or purchase of a Factor Risk Model (either a fundamental factor model or maybe a simpler PCA style model). New issues and ticker changes creates an additional operational headache for these algorithms.
\item Optimization linked schedules are very rigid and to avoid significant breaches of the provided constraints the algorithm has to follow the schedule quite closely. This means that the algo usually cannot leverage additional opportunistic liquidity. In a market structure like the US where 30-40\% of the liquidity is in dark pools this makes these algorithm not very effective in sourcing liquidity. Some effort in incorporating dark pools int he optimization problems were attempted in (Kratz, Sch{\"o}neborn) (2012) \ref{krasch} but no production algorithm the authors know of have actually solved this problem effectively. \textnote{Max, did you guys ever finalized Portfolio Aqua to deal with this? If so want to adjust this?}
\end{itemize}


\section{Other Algorithmic Trading Usecases}
We completed this incredible whirlwind tour of the technology stack present in 

\section{ATS Infrastructure}
\subsection{Matching Engine}
\subsection{Client Tiering and other Rules}

