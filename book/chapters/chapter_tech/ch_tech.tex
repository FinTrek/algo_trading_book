% !TEX root = ../../book.tex
\chapter{The Technology Stack\label{chap:ch_tech}}

There are a broad spectrum of trading infrastructures and operations but none, we think, is as broad and complex as the one in a large electronic trading execution operation in a large Broker-Dealer. So we use that as a template for our exploration of the technology stack that is necessary to support such an operation. We start by reviewing the end to end flow of information and provide some detail on various components. As previously discussed all large ET businesses also operate their own ATS so we will briefly look at the technology setup needed to support that use case as well.


\section{From client Instruction to Trade Reconciliation}

TODO Build an end-to-end diagram


Figure~\ref{fig:pexchrate} shows the end to end diagram of a hypothetical trading infrastructure. We can use this diagram to follow the full lifecycle of a client order and the main components of the infrastructure involved. \\


\noindent\textbf{Client Side:} The trader enters an algorithmic order into the Execution Management System (EMS), a specialized trading infrastructure that integrates the internal systems and provides tools for institutional traders to manage the day to day trading needs. EMS are most often vendor products with Ez Castle\footnote{\url{https://www.ezesoft.com/}}, Portware\footnote{\url{http://www.portware.com/}} and FlexTrade,\footnote{\url{https://flextrade.com/}} which are some of the most popular options. Many large buy-side trade organizations have more complex needs and often have an homegrown solution.


The trader normally selects from a drop-down menu, a particular provider and an Algo strategy and  chooses the desired parameters. Brokers who want to expose their execution into an EMS need to certify with the EMS by providing a FIX Specification Document that highlights what strategies are available and what parameters are available for each strategy and the validation information about the parameters. The EMS then integrates this into their system and exposes them in the front end for the trader to chose. Financial Information eXchange (FIX) and is a standard communication protocol specifically designed for financial applications.\footnote{\url{https://www.fixtrading.org/}} Once the order is submitted, the EMS uses a preconfigured network connection to transfer the order to the broker. This session is created as part of the \emph{Client Onboarding} step, a quite laborious process to setup a new client relationship, configures the client financial limits and controls. \\


\noindent\textbf{Inbound Gateway:} We are now on the broker-side where the order is received by and Inbound gateway. First, it is ensured that the message is a valid order and has all the necessary fields. Next step is to perform a set of risk and credit checks to ensure that the order is within the specified risk limits and the client's overall exposure, the maximum notional the client is allowed to trade and have in the market at any point in time. If mismatch in these validation steps is detected the order is rejected and sent back to the client. \\


\noindent\textbf{Order Management System and Order Enrichment:} Once the order is validated, the order is created within the broker dealer infrastructure. Order life-cycle is quite complicated and is nuanced. It is critical that the state of the order is always up to date and transitions in states are to be carefully managed. The role of creating and managing the state of the order is fulfilled by an Order Management System (OMS). These infrastructures are at the heart of any trading operation and they are often built around a Bus-based architecture, a software paradigm where loosely coupled components communicated with each other over a messaging middleware. The advantage of this approach is that other ancillary components within the infrastructure can ``Listen in'' on the messages and use the information to transmit to other systems or collect data for trade reporting and analysis.


One step often performed in this layer is Order Enrichment. This is a step that adds additional, lower level parameters that adjust and customize the execution behavior for the specific client/algorithm/parameters triplet. This step also translates nuances between the instructions the client sends and what and how the execution system understands. This is often accomplished by a specialized \emph{Rules Engine} a software library that allows a set of rules to be applied to an order. The rules are then reevaluated after any modification until no additional rules are triggered. \\


\noindent\textbf{Execution Strategy Stack:} As the famous quote goes: ``Is where the magic happens!'' The order reaches the software component where the Algo is actually executed. We delve into more details on the execution stack in the next section so here we assume that the strategy is initialized and starts executing. The strategy associated with the order contained in the OMS is often called the ``Parent Order" and as consequence of the strategy logic one or more ``child'' orders  are created in the OMS and sent forward for submission to one or more trading venues. Before the order is actually forwarded, it passes through an additional control layer, that ensures the strategy does not violate the risk limits and speed bumps, general term used for limits that prevents the strategy from trading too fast or too aggressively or from sending too many child orders, etc.. \\


\noindent\textbf{Outbound Gateway:} The child orders are received by another piece of infrastructure that is responsible for actually transmitting the orders to the market: the Outbound Gateway. All venues support additional protocols to communicate with market participants. Essentially all of them support a FIX protocol but in most cases they also support a much faster ``native'' protocol that encodes the instructions in a compressed binary protocol. The role of the Outbound Gateway is to connect to various venues and then act as a translation layer from the internal representation in the OMS to the external representation of the specific protocol implemented by the venue. The outbound gateway also listens in to the connection callback to capture any asynchronous event coming from the venue like order insert/cancellation acknowledgements and executions events, and updates the state of the child order representation in the OMS. \\


\noindent\textbf{Notifications to the Client:} As the strategy executes orders in the market, the OMS keeps the state of the parent order up to date earlier with each execution or with periodic updates back via the inbound gateway back to the client's EMS that updates its own state and provides feedback to the trader that the strategy is executing and what the average prices achieved, and other analytics necessary for the trader to understand how well the strategy is executing. The inbound control layer is also kept up to date so that the total state of all client orders is accounted for if/when a new order from the same client is received. \\


\noindent\textbf{Middle and Back Office:} We are almost done. The step above completes the real-time feedback loop from the client through the executing strategy to the market and back. The rest of the processing is in most cases done offline by a set of infrastructures commonly referred as ``Middle and Back Office Systems.'' These systems play a critical role in the business and regulatory side of trading. They are arguably the most important piece of the puzzle without which no trading operation could function. As we previously discussed, performance of an algorithm is really just the cherry on the trading cake. Without a trade knowing what has been traded and what current position is, without the confidence that a once a buy order is executed, within the $T+2$ settlement period (more on this below) the shares the trader just bought will be in the trader's account, nothing else would matter. Describing in detail the complexity and subtleties of the process that ensures that once an order is executed the shares or the monies are in the right account at the right time is well beyond the scope of this book. But the authors feel it is important that the reader has at least some understanding and a high degree of respect of the critical role these systems, the hard working people operating them handling the myriad of exceptions and trade breaks play in the industry.


To start let us quickly review the process by which share ownership is transferred between large institutional sellers and the buyers. Stock ownership is almost never in paper form but very likely stored as a book entry in a computer at a place called Depository Trust Company (DTC). This firm act as the main custodian for all the shares that can freely trade on an exchange. Brokers will have an account at the DTC that keeps custody of all the shares the broker holds on behalf of so clients from the DTC perspective the broker is the owner of the shares. The process of transferring ownership goes through two steps:
        \begin{itemize}
        \item Clearing: It is the process of updating the accounts of the trading parties and arranging for the transfer of money and securities. In most cases this step goes through yet another intermediary, a Central Clearing entity that provides various services in particular all trade aggregations so only the net shares across all trades are  transferred.
        \item Settlement: It is the process of actual exchange of securities for cash. This in most cases (there are always exceptions in a complicated process) happen on $T+2$.
        \end{itemize}
The DTC then also handles other functions like the handling on stock dividends, etc..


It is obvious to note that an investor cannot trade directly with each other but can only trade through an intermediary, a Broker. Large investors often trade with multiple brokers but engage one (and some time more than one) as a Prime Broker where they centralize their balances. Prime brokers provide all set of services, e.g. stock lending, financing. The client will setup one or more often many accounts with the prime broker (sometimes thousands depending on the complexity of the operations) where they record which shares registered in DTC do actually belong to the client.


Let us finally turn to the main topic. In executing, broker middle and back office operations are segregated into two distinct parts and are managed independently: 
        \begin{itemize}
        \item The Market Side: Deals with the interaction with the various exchanges  and the clearing house and ensure that there is a perfect match between what the broker thinks what it has been traded and what actually was traded, at what price and how, i.e. taking vs. providing. This is then used to actually determine who the broker needs to pay the various exchanges for their execution fees (or get paid by them if a rebate was earned).
        \item The Client Side: Handles the clearing and allocation of all traded shares to various accounts based on the client instructions (mostly in electronic format but sometimes via email) and ensure that their records match exactly what the client has and what their prime brokers have in their records and handle the reconciliation of frequent breaks.
        \end{itemize}


Each side is usually separated in a set of Middle office functions that handles the booking and allocations and the confirms with the client/exchanges and a back office that manages issues around settlement.
The back office also handles the regulatory burden of  managing the trade reporting process to the TRF/ACT and the OATS process that sends to FINRA the full life-cycle of the order. As the reader can now appreciate, the role of this function is absolutely critical to the correct functioning of a trading operation. The middle and back-office teams are truly the unsung heroes of this endeavor. Without them, what we do would not matter one single bit.



\section{Algorithmic Trading Infrastructure}

TODO Add a Diagram with the main components


This section details a somewhat generic approach for algorithmic trading infrastructure. Different providers have different approaches to this. For instance in some platforms, the core strategy is in one infrastructure but the order routing part is a separate (albeit similar) infrastructure. Others combine the two together, others separate scheduling and order placement/routing components. The below simplified discussion assumes an all-in-one approach.


The core of an Algorithmic Trader infrastructure is a software framework generically called ``Strategy Container.'' This facility sits in-between the low level OMS related functionality and the strategy and provides a set of abstractions and interfaces to simplify the development of a trading algorithm. When a new order event is received from the OMS the strategy container reads and validates the instructions like the name of the strategy and the necessary and additional parameters. If this is a valid new order instruction it usually has to pass through and additional layer of controls to ensure that various variables such as order quantity or limit price are within the required boundaries. Once this validation step is completed the strategy container instantiates the code that encodes the particular strategy, initializes it with the specific parameters. It will also connect the algorithm to the necessary services that the algorithm needs for implementing the strategy. Let us look at some of the main services and related infrastructures. \\


\noindent\textbf{Static Data Services:} An Algorithmic strategy requires a slew of reference data and other static data to operate. First and foremost, the particular instrument the strategy is trading but also other information like the primary exchange is, open/close time of the exchanges, etc.. It will also require the normalizing analytics as well as any calibration parameters for various models. These static data services abstract the sources of the underlying data that could be databases, flat files and provide calls to other systems. \\


\noindent\textbf{Market Data Facility:} Access to real-time market data is the most important component of any trading algorithm. Strategy container is usually provided with two ways of accessing this data: via a data cache that is continuously kept up to date by an underlying thread, and via callback. The data cache also continuously updates additional core analytics used by the strategy such as total traded volume (filtered for specific condition codes) and others. A natural question is how do we actually get real-time market data? Connected to our market data facility is one of the most demanding and expensive pieces of infrastructure in our whole stack: the Market Data Plant. This is worth a small detour for a brief discussion. \\


\noindent\textbf{Market Data Plant:}  Any serious trading operation, in particular in the post NMS and the OPRS era requires to access protected quotes, to subscribe and deliver direct market feeds from all protected exchanges. For most use cases top of book (Level~I) data is not going to be sufficient. It is necessary to subscribe to at least Level~II or even better Level~III data. Each exchange family (and sometime even within one) has it's own proprietary multicast protocol. Also very important information like trade condition codes are also proprietary and are to be normalized. For each Level~III market data feed, a book building library is needed that interprets all the multicast events and updates the internal state of the book. Doing this in a very efficient way that can withstand market data spikes without dropping any multicast packet is not an easy task. It requires some skills and an excellent networking and computer infrastructure, something that is not expensive to build and maintain. More often, trading operations rely on third party vendor that often provide a hybrid software and hardware solution to handle the whole operation. Some of the vendors in this space are: Reuters,\footnote{\url{https://financial.thomsonreuters.com/en.html}} Redline Trading Solutions,\footnote{\url{https://www.redlinetrading.com/}} Exegy.\footnote{\url{https://www.exegy.com/}} Over and above the significant costs of operations, exchanges have increased the cost of subscription market data. This is a significant component of the overall cost of running a trading operation (in the the six digit range!). \\


\noindent\textbf{Outbound Order Interface:} Now back to the discussion of the core services. The last core set of abstractions has to do with managing outbound child orders. This facility provides the strategy with the state of all outstanding child orders and ability to create, cancel and amend them in an asynchronous way and manage any exceptions like for example, canceling an order that has just executed on exchange but the fill event has not yet reached the OMS. \\


\noindent\textbf{Main Strategy Loop:} TODO Add a diagram of the main loop, with state cache, etc?


We come to the \emph{core of the Core} of the whole infrastructure. While there are many ways to write a trading strategy a pattern has emerged that can be describe as a standard approach. It is sometimes referred to as the Main Strategy Loop.\footnote{Some practitioners for example believe that the right conceptual framework for a strategy is that of a State Machine: Based on what event just happen there is a ``state transition'' in the next state and writing a strategy is essentially to model and implement these state transitions. Conceptually elegant, in practice this approach is extremely hard to pull off as there are a lot of side effects and co-dependencies that makes it hard to cleanly decompose.} The strategy container interacts with other working threads ensure that the state is always kept up to date. The strategy flow is encoded in essentially are one stateless function. This is called on when either a timer, any major event such as quote change/trade/execution, etc., or just a tight loop occurs. This function performs three phases:
        \begin{enumerate}
        \item Understand where the strategy is at any point in time by recovering the state from the state cache with all the information it needs. How many shares have been traded, and how many, based on the target strategy, should be traded and where should the traders stand in the near future. 
        \item Based on the full information set and the current state of the outstanding child orders in the market, determine which orders to cancel, which to amend, and which to submit.
        \item Send these instructions to the Outbound Order Interface. In some cases Step 2 just calculates the correct exposure it wants in the market and it is the role of this step to optimally decide if and how this should be implemented by either submitting additional orders or amending the existing ones, etc.
        \end{enumerate}
After performing these three steps, the main loop will exit and repeat at some time later. Since the approach is Markovian, meaning only the existing state matters; if the strategy is called again immediately after, it will most likely make the same decisions it just made and determine in Step 2 that no action is necessary. Thus, the main loop simply exits. \\


\noindent\textbf{Additional Infrastructures:} Depending on the complexity and sophistication of the operation, the technology stack is often supplemented by additional components that serve certain specific roles. Let us look at a few of the most common ones. \\


\noindent\textbf{Realtime Analytics Engine:} More and more, algorithmic trading products have become highly sophisticated and dynamic and adapt as market conditions change. The normalization variables and signals are adjusted every single tick or every few trades to provide the most uptodate picture of the immediate past and future market state. These analytics are built within the current Algo engine but it is not only more cumbersome but also is inefficient. In almost every single case, one process cannot handle the computing load to run thousands of algorithms on all symbols. The infrastructure is striped in multiple processes handling a subset of symbols. Per symbol analytics could still be built within each process but many analytics, like for example computing the real-time price of an index midpoint, it requires that the data on hundreds of symbols need to be computed in the same process. For each symbol, this creates large duplication, reducing the effectiveness of striping. Additionally with the more analytics, more load on the process is created that is used to handle the algorithmic decisions.


For these and other reasons of convenience with calculating real-time analytics, it is often delegated to a dedicated infrastructure. This makes it easier to build additional analytics and to combine existing ones with more complex signals. 


\noindent\textbf{Algorithm Switching Engine:} Most execution strategies have limited flexibility to change behavior in particular situations and opinionated traders may have strong beliefs that a different trading approach might be warranted in certain situations. Historically that trader may work with one favorite broker to build a customized strategy but that usually took a long time and then the trader could only use this approach with only one broker. To provide simple dynamic customization, brokers have started offering Algo of Algos functionality around what is commonly called an Algo Switching Engine. This Engine is a combination of a simple strategy container and a limited real-time analytics engine, and where an Algo is represented by a simple set of if-then-else rules. The condition  part of these rules will be simple combinations of the available signal and the action part will define an particular algo with associated parameters. Once the order is received by the switching engine the rules are evaluated and an order for the specified strategy is sent to the strategy container. The rules are then re-evaluated every few seconds and if the conditions change the switching engine will send a cancel/replace message to the strategy container to change the underlying strategy. 


With this simple approach, one can build very sophisticated dynamic algorithms thus offering almost unlimited capabilities and have made algo switching engine very popular both with traders and with execution consultants that often differentiate themselves by smart usage of the switching engine. While Algo Switching Engine are a very powerful tool, it has several meaningful downsides that one should consider:
\begin{itemize}
\item The rules that make up a custom strategy encode particular beliefs on the value of the particular triggering condition. If that condition had true predictive power an alpha signal would be better used in a more nuanced way within the strategy rather than as a blunt instrument of an algo switching instruction. If the condition is actually, as most often the case, not predictive then the realized performance of the strategy will be, in general, poor by the customization. Thus while in principle it is a good idea in practice it should be discouraged unless its real value can be measured. This would make algo switching engine ideal as a temporary exploratory tool rather than the overused tool it currently is.
\item  Algorithms are usually poor at persisting state across a cancel/replace transition. This usually implies that all state of the algorithm is lost and a completely new order is created. Algos also perform relatively poor at the beginning of the order as various analytics and execution bands exhibit poor performance when initialized (e.g. participation bands unless handled correctly have a discontinuity at 0). Frequent use of switching transitions will have severe negative impact on algo performance.
\item Once a trader is onboarded into a custom strategy any Post Trade support by the broker since the optionality embedded in the switching to a large extent relieves the broker from any performance obligation as the performance will be largely driven by the switching rules. Additionally, these orders cannot be used by brokers to measure and improve their own performance since the individual slices will be hopelessly biased by the switching rules
\end{itemize}


As one can glean from the above treatment of algo switching engine, the authors are not big fans of this approach and believe that this evolution has resulted in decreased quality of execution algorithms. The popularity of these approaches is still quite high and we believe currently the approach is currently overused. However, there are signs that this popularity is slowly waning as clients are starting to rely on the broker to fine tune the behavior of the strategies. \\


\noindent\textbf{Portfolio Algorithm Engine:} As we discussed in Chapter~\ref{chap:ch_exec_models}, some algorithmic strategies involved the coordinated trading of a whole portfolio of instruments. In this section, we briefly look at the technology infrastructure needed for these strategies and some related common issues. The problems start at the interaction between the FIX infrastructure and the client's EMS. Until recently, Basket trading was  never well supported by various EMSs making the certification of a portfolio strategy, a potentially very fiddly proposition as it requires supporting multiple approaches. Where basket trading is not supported, firms have resorted to simple ways like considering every portfolio order sent to that algo within a certain timeframe, or linked to a portfolio ID parameters. \textnote{Need to confirm that this is still the case}


The portfolio algorithms infrastructure is not dissimilar to the one used in Algo Switching but with the added complexity. It now needs to manage the schedule for multiple orders. This algorithms will create a set of linked trajectories, often through some form of portfolio optimization process that implements the specific objective function. Once the trajectories have been generated the algorithm will, in most cases, use one or more existing  algos possibly modified to adapt to the portfolio infrastructure.  Certain algo implementations send short time slices to a TWAP algo (essentially transforming the trajectory into a piece-wise linear curve) while other implementations may use a modified VWAP style algo that supports a parameter that specifies the trajectory to be used. Once the portfolio starts trading, market condition changes or realized deviations of the portfolio shape (usually as an impact security measure the single slice will have a max POV constraints applied, which if triggered, they will distort the original intent) usually require the algorithm to re-optimize and the individual slices are amended to account for the new schedule.


This workflow is already quite complex but there are several additional mechanical and numerical issues that further complicate matters making portfolio algorithms really hard to pull off successfully. Here are several practical issues:
\begin{itemize}
\item Among the portfolio IS style algorithms, the most common algo is usually implemented by a QP (Quadratic Programming)\footnote{\url{https://en.wikipedia.org/wiki/Quadratic_programming}} optimization problem where the schedule is discretized into bins representing the shares to be traded in a certain period. For large portfolios common in fund rebalancing and for relatively fine grained time bins, the number of variables that need to be used explodes pretty fast (in particular formulating these problems require use of several auxiliary variables to manage cost constraints, etc.). For example, a 1000 stock portfolio with 5m bins  sent to a whole day Portfolio IS algorithm would have to solve a problem with more than 100k variables. This often results in a very slow  optimization cycle that can take hundreds of seconds making it too slow for effective utilization (no client would want to wait several minutes to see the first trade) and too compute intensive for a scalable operation, e.g. typically a dozen such portfolios arrive at the beginning of the day. To solve this problem effectively requires nontrivial numerical methods that makes the actual optimization problem very difficult to solve.

\item Essentially, all such algorithms are discretized in time bins, and hence synchronizing the re-optimization to correctly line up is not easy and often leads to purely mechanical distortion of the realized schedule even in very simple settings (e.g. the tendency of the orders to be behind schedule leads to the unexecuted quantity to be reallocated to the remaining bins causing a significantly stretched execution schedule).

\item The portfolio level optimization problem requires a variance-covariance matrix for all instruments in the portfolio. As discussed, the universe of security that are traded with algos is usually very large and to have stable covariance matrix for such universe requires the creation or purchase of a Factor Risk Model (either a fundamental factor model or maybe a simpler PCA style model, see Chapter~\ref{ch:amp}). New issues and ticker change creates additional operational issues for these algorithms.

\item Optimization linked schedules are very rigid and to avoid significant breaches of the provided constraints, the algorithm has to follow the schedule quite closely. This means that the algo usually cannot leverage additional opportunistic liquidity. In a market structure like the US where 30--40\% of the liquidity is in dark pools this makes these algorithm not very effective in sourcing liquidity. Some effort in incorporating dark pools into the optimization problem was attempted in Kratz, Sch\"oneborn (2012)~\ref{kratzschon} but no production algorithm the authors know of have actually solved this problem effectively. \textnote{Max, did you guys ever finalized Portfolio Aqua to deal with this? If so want to adjust this?}
\end{itemize}



\section{HFT Infrastructures}

We have completed the whirlwind tour of the technology stack that one would find in a standard execution business. As mentioned we did  that because it is probably the most complex and broad use case. For other setups, some of the components my be very similar, but others may differ. There are probably a dozen different configurations and discussing all of the nuances in detail is well beyond the scope of this brief chapter but as a comparison we take a quick look at the other extreme, an HFT infrastructure.


In broad terms, HFT setup is in some ways a lot simpler while in other technical complexities beyond what are usually found in execution. To begin with, HFT strategies are not driven by instructions; thus, there is no need for all inbound connectivity, OMS, etc..  Additionally in order to achieve the maximum speed and the lowest  ``tick-to-trade" latency,\footnote{The total time it takes from a new market data event happening on exchange (tick), to that tick being received by the trading engine which in turn generates an order, to that order reaching the exchange} (in general) the technology stack is condensed to run all in the same process. Direct market data feeds and direct exchange connectivity are just components of the same process reducing the complexities of a separate ticker plant, communication bus, and connectivity layer all together. 


If the trading operation sits outside a broker-dealer then the trade has to go through a third party broker-dealer via a sponsored access agreement. This usually requires that the trading system will connect to a broker provided appliance or software layer and through that connect to various venues through the native market access protocols. In most cases, the trading infrastructure is co-located in the same data center as the exchange(s) where that strategy will operate.


Finally, the other main difference is the need for the infrastructure to connect to a position keeping system to incorporate the current inventory as part of the decision making. Usually these positions are also computed internally based on start of days positions in order to have an internal verification with additional controls if the positions calculated internally deviate from the position calculated by the position keeping system.


What other areas then do the technical complexities exist beyond standard infrastructures? It is often driven by the pursuit of absolute speed and throughput. This implies that it requires that having deep understanding of the networks, hardware and software, utilizing all the options in the book to squeeze additional nanoseconds. This includes specialized network switches, CPU overclocking, kernel modifications, use of assembly/machine language for the most latency sensitive parts of the codebase. It could also include setting up connectivity to far away exchanges, most notably the CME exchange where the S\&P 500 Futures is traded using proprietary networks like spread networks (now part of Zayo Group)\footnote{\url{https://www.zayo.com/private-dedicated-networks/}} or via Microwave Towers like what is provided by the ICE.\footnote{\url{https://www.theice.com/market-data/connectivity-and-feeds/wireless/chicago-to-new-jersey}} Recent evolution of these ultra low latency setups involve the use of Fileld-Programmable Gate Array chips (FPGA)\footnote{\url{https://en.wikipedia.org/wiki/Field-programmable_gate_array}} for all in one appliances where  feeds, connectivity and strategy are essentially programmed in hardware.


\section{ATS Infrastructure}
\subsection{Matching Engine}
\subsection{Client Tiering and other Rules}



















