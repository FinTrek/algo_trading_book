% !TEX root = ../../book.tex
\chapter{Advanced Topics\label{ch:ch_advanced}}

In this chapter,\label{in:adv_model} we want to discuss some advanced methods that are applicable in the context of trading. The mechanics of actual trading (entry/exit) decisions call for trading the market movements in real time and the tools provided in this chapter would be quite useful to better track and understand the market changes. This chapter is broadly divided into two main areas; the first area covers topics which are extensions of models discussed in Chapter~\ref{ch:ch_uvts} in the low frequency context. Although the state-space modeling is discussed in the context of ARIMA, its scope is much wider. We also outline the established methodology for studying regime switching, but indicate the topics of further research where predicting ahead when the regime shift is likely to occur using signals from the past is of great interest. This is followed by a discussion on using volume information to study the price volatility. With the increased exploitation of price behavior data, it has become necessary to look for additional trading signals. We present a theoretical model that shows that there is correlation between volume traded and volatility when there is information flow. The second area of this chapter focuses on the models from point processes to study the higher frequency data. Here our approach has been to provide an intuitive discussion of main tools; as readers will realize, the trade data is quite noisy due to market friction, The formal parametric models do not perform well. Thus there is a great deal of scope to do research in non-parametric empirical modeling of arrivals and cancellations in the limit order market. 



% State-Space Modeling
\section{State-Space Modeling\label{sec:state_space}}

The state-space models were initially developed by control systems engineers to measure a signal contaminated by noise.\label{in:state_space} The signal at time ``$t$'' is taken to be a linear combination of variables, called state variables that form the so-called state vector at time, $t$. The key property of the state vector is that it contains information from past and present data but the future behavior of the system is independent of the past value and depends only on the present values. Thus the latent state vector evolves according to the Markov property. The state equation is stated as,
	\begin{equation} \label{eqn:2zt}
	Z_t = \Phi_tZ_{t-1} + a_t
	\end{equation}
and observation equation as,
	\begin{equation} \label{eqn:2yt}
	Y_t = H_tZ_t + N_t,
	\end{equation}
where it is assumed that $a_t$ and $N_t$ are independent white-noise processes; $a_t$ is a vector white noise with covariance matrix $\Sigma_a$ and $N_t$ has variance $\sigma_N^2$. The matrix $\Phi_t$ in \eqref{eqn:2zt} is an $r \times r$ transition matrix and $H_t$ in \eqref{eqn:2yt} is a $1 \times r$ vector; both are allowed to vary in time. In engineering applications, the structure of these matrices are governed by some physical phenomena. 


The state space form of an ARIMA$(p,d,q)$ process $\Phi(B)Y_t= \theta(B)\epsilon_t$ has been extensively studied (see Reinsel (2002)~\cite[Section~2.6]{2002reinsel}). Observe if $\Phi_t \equiv \Phi$ and $H_t \equiv H$ for all $t$ then the system \eqref{eqn:2zt}--\eqref{eqn:2yt} is said to be time-invariant. A nice feature of expressing a model in state-space form is that an updating procedure can be readily used when a new observation becomes available to revise the current state vector and to produce forecasts. These forecasts in general tend to fare better. 


For the state-space model, define the finite sample estimates of the state vector $Z_{t+1}$ based on observations $Y_t, \ldots, Y_l$, as $\hat{Z}_{t+l|t} = \Ex[Z_{t+l} \;|\; Y_t,\cdots,Y_l],\text{ with } V_{t+l \;|\; t} = \Ex[(Z_{t+l} - \hat{Z}_{t+l \;|\; t})(Z_{t+l} - \hat{Z}_{t+l \;|\; t})']$. A convenient computational procedure, known as the \emph{Kalman filter equations},\label{in:kalman} is used to obtain the current estimate $\hat{Z}_{t\;|\;t}$, in particular. It is known that (see Reinsel (2002)~\cite{2002reinsel}), starting from some reasonable initial values $Z_0 \equiv \hat{Z}_{0 \;|\; 0}$ and $V_0 \equiv V_{0\;|\;0}$, the estimate, $\hat{Z}_{t \;|\; t}$, can be obtained through the following recursive relations:
	\begin{equation}\label{eqn:2hatz}
	\hat{Z}_{t \;|\; t} = \hat{Z}_{t \;|\; t-1} + K_t(Y_t - H_t \hat{Z}_{t \;|\; t-1}),
	\end{equation}
where $K_t= V_{t \;|\; t-1} H_t'[H_t V_{t \;|\; t-1} H_t' + \sigma_N^2]^{-1}$ with $\hat{Z}_{t \;|\; t-1} = \Phi_t \hat{Z}_{t-1\;|\; t-1},V_{t \;|\; t-1} = \Phi_t V_{t-1\;|\; t-1} \Phi_t' + \Sigma_{a}$ and $V_{t \;|\; t} = [I - K_t H_t] V_{t \;|\; t-1} = V_{t\;|\; t-1} - V_{t\;|\; t-1} H_t' [H_t V_{t \;|\; t-1} H_t' + \sigma_N^2]^{-1} H_t V_{t \;|\; t-1}$ for $t= 1,2, \ldots$.


In \eqref{eqn:2hatz}, the quantity $\varepsilon_{t \;|\; t-1}= Y_t - H_t \hat{Z}_{t \;|\; t-1} \equiv Y_t - \hat{Y}_{t \;|\; t-1}$ at time $t$ is the new information provided by the measurement $Y_t$, which was not available from the previous observed (finite) history of the system. The factor $K_t$ is termed the ``Kalman gain'' matrix. The filtering procedure in \eqref{eqn:2hatz} has the recursive ``updating'' form, and these equations represent the minimum mean square error predictor property. For example, note that $\Ex[Z_t \,|\,Y_t,\ldots,Y_1]= \Ex[Z_t \,|\,Y_{t-1},\cdots,Y_1] + \Ex[Z_t \,|\, Y_t - \hat{Y}_{t \;|\; t-1}]$, since $\varepsilon_{t \;|\; t-1} = Y_t - \hat{Y}_{t \;|\; t-1}$ is independent of $Y_{t-1}, \ldots, Y_1$. Note from \eqref{eqn:2hatz}, the estimate of $Z_t$ equals the prediction of $Z_t$ from observations through time $t-1$ updated by the factor $K_t$ times the innovation $\varepsilon_{t|t-1}$. The quantity $K_t$ can be taken as the regression coefficients of $Z_t$ on the innovation $\varepsilon_{t|t-1}$, with $\var(\varepsilon_{t\;|\; t-1}) = H_t V_{t\;|\; t-1} H_t' + \sigma_N^2$ and $\cov(Z_t,\varepsilon_{t\;|\; t-1})= V_{t \;|\; t-1} H_t'$. Thus, the general \emph{updating relation} is $\hat{Z}_{t\;|\;t} = \hat{Z}_{t\;|\; t-1} + \cov(Z_t,\varepsilon_{t\;|\; t-1}) \{ \var(\varepsilon_{t\;|\; t-1}) \}^{-1} \varepsilon_{t\;|\; t-1}$, where $\varepsilon_{t \;|\; t-1} = Y_t - \hat{Y}_{t\;|\; t-1}$, and the relation in $V_t$ is the usual updating of the error covariance matrix to account for the new information available from the innovation $\varepsilon_{t\;|\; t-1}$. Forecasts of future state values are available as $\hat{Z}_{t+l\;|\;t} = \Phi_{t+l} \hat{Z}_{t+l-1\;|\;t}$ for $l = 1,2,\ldots$. 


Note for ARIMA models with $Y_t = HZ_t$ and $H = [1,0,\ldots,0]$, this Kalman filtering procedure provides an alternate way to obtain finite sample forecasts, based on data $Y_t,Y_{t-1}, \cdots, Y_1$, for future values in the ARIMA process, subject to specification of appropriate initial conditions to use in the terms in \eqref{eqn:2hatz}. For stationary $Y_t$ with $\Ex(Y_t)= 0$, the appropriate initial values are $\hat{Z}_{0 \;|\;0} = 0$, a vector of zeros, and $V_{0 \;|\;0} = \cov(Z_0) \equiv V_*$, the covariance matrix of $Z_0$. Specifically, because the state vector $Z_t$ follows the stationary vector AR(1) model $Z_t = \Phi Z_{t-1} + \psi \varepsilon_t$, its covariance matrix $V_*= \cov(Z_t)$ satisfies $V_*= \Phi V_* \Phi' + \sigma_{\varepsilon}^2 \Psi \Psi'$, which can be readily solved for $V_*$. For nonstationary ARIMA processes, additional assumptions need to be specified. The ``steady-state'' values of the Kalman filtering procedure $l$-step ahead forecasts $\hat{Y}_{t+l\;|\;t}$ and their forecast error variances $v_{t+l\;|\;t}$ will be identical to the expressions given in Chapter~\ref{ch:ch_uvts}, $\hat{Y}_t(l)$ and $\sigma^2(l) = \sigma^2 (1 + \sum_{i=1}^{i-1}\psi_{i}^2 )$. 


An alternative set-up, alternative to is to assume that the deviations in the observation and transition equations are related. With the observation equation stated in \eqref{eqn:2yt}, the transition equation is modified as 
	\begin{equation} \label{eqn:223zt}
	Z_t = \Phi Z_{t-1} + \alpha N_t.
	\end{equation}
This model studied in Ord, Koehler, and Snyder (1997)~\cite{ord1997estimation} with a single source of error $(N_t)$ is shown to be closely related to various exponential smoothing procedures. There are other formulations of state-space models, such as structural models by Harvey (1989)~\cite{harvey1989kalman} and dynamic linear models of West and Harrison (1997)~\cite{west1997} that are found to be useful for model building.


We provide two examples to illustrate the application of the models \eqref{eqn:2zt} and \eqref{eqn:2yt} in the broader context of this book. 


\begin{ex}[Generalized Local Trend]
The slope in a local linear trend is a random walk. Specifically,
	\begin{equation} \label{eqn:randomwalk}
	\begin{aligned}
	p_t&= \mu_t + e_t \\
	\mu_t&= \mu_{t-1} + \delta_{t-1} + \eta_{0t} \\
	\delta_t&= \delta_{t-1} + \eta_{1t}
	\end{aligned}
	\end{equation}
Here the trend is composed of level `$\mu_t$' and slope `$\delta_t$'. The slope `$\delta_t$' when written as 
	\begin{equation} \label{eqn:slopewritten}
	\delta_t= D+ e(\delta_{t-1} - D)+ \eta_{1t}
	\end{equation}
in AR(1) form is sometimes known as generalized local linear trend. These models can be written in the form of \eqref{eqn:2zt} and \eqref{eqn:2yt}; the updating process given in \eqref{eqn:2hatz} can be used to accommodate a wide range of models. \xqed
\end{ex}


\begin{ex}[Price Discovery in Multiple Markets]
Menkveld, Koopman and Lucas (2007)~\cite{menkkoop} study the price discovery process for stocks that are cross-listed in two different markets via state-space models. The question if the trading location matters more than business location is of interest to finance researchers. Consider a stock traded in Amsterdam and in NY exchanges. Because of the time difference, there is only an hour of overlap in trading. Ignoring this aspect, write the two-dimensional price vector as
	\begin{equation}
	\begin{aligned}
	p_t&= \mu_t + e_T \\
	&=\mu_t + \theta(\mu_t - \mu_{t-1}) + e_t
	\end{aligned}
	\end{equation}
as the observational equation with delayed price reaction and the state equation is
	\begin{equation}
	\mu_t= \mu_{t-1} + \xi_t.
	\end{equation}
Here $\epsilon_t$ can represent a more general covariance structure such as the factor model where a common factor that can influence both prices could be easily accommodated. The above can be written in the form of \eqref{eqn:2zt} and \eqref{eqn:2yt} with appropriate choice of `$\Phi$' and `$H$' matrices. For details see Section~2.4 of Menkveld et al. (2007)\cite{menkkoop}. The main advantage of state-space models is that they accommodate the possibility of slowly varying parameters over time, which is more realistic in modeling the real world data. \xqed
\end{ex}


The estimation of model parameters can be done via likelihood function. For this and how VAR models discussed in Chapter~4 can be written in the state-space form, see Reinsel (2002)~\cite{2002reinsel}.



% Regime Switching and Change-Point Models
\section{Regime Switching and Change-Point Models}


It has been long noted in finance that there are two kinds of recurrences in stock prices: underreaction and overreaction to a series of good or bad news.\label{in:regime} The securities that have a good record for an extended period tend to become overpriced and have low returns subsequently. Barberis, Shleifer and Vishny (1998)~\cite{vishny} present a model where investor believes that the returns can arise from one of two regimes although returns follow random walk: mean-reverting or trending. The transition probabilities are taken to be fixed and the regime is more likely to stay the same rather than to switch. But the investor's beliefs are updated as and when the returns data are observed. It is presumed that in many instances, we may not discern these shifts directly whether or when they occur but instead can draw probabilistic inference based on the observed behavior posthumously. Hamilton (2016)~\cite{jdham} reviews the area of regime-switching applied in several contexts in macroeconomics, building upon the elegant model introduced in Hamilton (1989)~\cite{89ham}. Here in this section, we will cover only studies that are relevant to finance applications, particularly relevant to trading that exploit anomalies. 


Consider the model,
	\begin{equation} \label{eqn:modelham}
	y_t = \mu_{s_t} + \epsilon_t, \quad s_t= 1, 2,
	\end{equation} 
where $y_t$ is the observed variable, such as asset return and $s_t$ represents two distinct regimes and $\epsilon_t$'s are i.i.d. $\sim N(0,\sigma^2)$. Let $\{ F_{t-1} \}$ be the information set available as of `$t-1$'. The transition between the regimes is taken to be Markovian,
	\begin{equation} \label{eqn:markprob}
	\text{Prob}(s_t= j \;|\; s_{t-1}= i, F_{t-1}) = p_{ij}, \quad i,j= 1, 2;
	\end{equation}
thus, leading to an AR(1) model for $\mu_{s_t}$ as
	\begin{equation} \label{eqn:must}
	\mu_{s_t} = \phi_0 + \phi_1 \mu_{s_{t-1}} + a_t,
	\end{equation}
where $a_t$ by definition can take four possible values depending upon $s_t$ and $s_{t-1}$. Note $\phi_0= p_{21} \mu_1 + p_{12} \mu_2$ and $\phi_1= p_{11} - p_{21}$. Also, note the model for $y_t$ in \eqref{eqn:modelham} is the sum of an AR(1) process and white noise, then from the aggregation Result~2 in Chapter~3, $y_t \sim$ ARMA(1,1); but because of discrete nature of `$a_t$', \eqref{eqn:modelham} is a non-linear process. Observe that the unknown parameters are $(\mu_1, \mu_2, \sigma,p_{11}, p_{22})'= \lambda$ and they can be estimated via maximum likelihood as follows: note $y_t \;|\; (s_t= j, F_{t-1}) \sim N(\mu_j,\sigma^2)$. The predictive density of $y_t$ is given as a finite mixture model,
	\begin{equation}\label{eqn:predden}
	f(y_t \;|\; F_{t-1})= \sum_{i=1}^2 \text{Prob}(s_t= i \;|\; F_{t-1}) \, f(y_t \;|\; s_t= i, F_{t-1})
	\end{equation}
and estimate of $\lambda$ is obtained by maximizing the likelihood function $L(\lambda)= \sum_{t=1}^T \log f(y_t \;|\; F_{t-1}, \lambda)$. Some useful results are: \twomedskip


\noindent Predicted regime: 
	\begin{equation} \label{eqn:predreg}
	\text{Prob}(s_t= j \;|\; F_{t-1})= p_{1j}\, \text{Prob}(s_{t-1}=1 \;|\; F_{t-1}) + p_{2j} \,\text{Prob}(s_{t-1}=2 \;|\; F_{t-1}).
	\end{equation}
\noindent Optimal forecast:
	\begin{equation} \label{eqn:optfore}
	\Ex(y_t \;|\; F_{t-1})= \sum_{i=1}^2 (\phi_0+\phi_1 \mu_i)\, \text{Prob}(s_{t-1}=i \;|\; F_{t-1}).
	\end{equation}
The forecast for `$k$' period ahead can be stated as:
	\begin{equation} \label{eqn:period}
	\Ex(y_{t+k} \;|\; F_{t-1}) = \mu+\phi_1^k \sum_{i=1}^2 (\mu_i-\mu)\, \text{Prob}(s_t=i \;|\; F_{t-1}),
	\end{equation}
where $\mu=\phi_0/(1-\phi_1)$.


The basic model has been extended in the literature to cover multiple regimes and to vector processes. Interested readers should refer to Hamilton (2016)~\cite{jdham} and references therein. Ang and Timmermann (2012)~\cite{timmerman} discuss a model that accounts for changes not only in the mean but also in variances and autocorrelations:
	\begin{equation} \label{eqn:varautoy}
	y_t= \mu_{s_t} + \phi_{s_t} y_{t-1} + \epsilon_t
	\end{equation}
where $\epsilon_t$'s are independent $\sim N(0,\sigma_{s_t}^2)$. Using the data on excess S\&P 500 returns, FX returns, etc., it is identified that there are volatility regimes, but no level shifts.


Hamilton (1990)~\cite{90ham} proposes using the popular EM (Expectation-Maximization) algorithm (Dempster, Laird and Rubin (1977)~\cite{dempster}) to obtain the maximum likelihood estimate, instead of recursive filter approach (see Hamilton (1989)~\cite{89ham}) to optimize the log-likelihood of \eqref{eqn:predden}, updated with $y_t$. Note by Bayes' Theorem
	\begin{equation} \label{eqn:bayes}
	\Prob(s_t= j \;|\; F_t) = \dfrac{\Prob(s_t= j \;|\; F_{t-1}) \, f(y_t\;|\; s_t=j, F_{t-1})}{f(y_t \;|\; F_{t-1})}
	\end{equation}
To get an estimate of $p_{ij}$ in step `$l+1$', use 
	\begin{equation}\label{eqn:hatpijl}
	\hat{p}_{ij}^{(l+1)}= \dfrac{\sum_{t=1}^{T-1} \Prob(s_t= i, s_{t+1}= j \;|\; F_t, \hat{\lambda}^{(l)})}{\sum_{t=1}^{T-1} \Prob(s_t= i \;|\; F_t, \hat{\lambda}^{(l)})}
	\end{equation}
and with these estimates obtain the ML estimates of the rest of the parameters ($\mu_1$, $\mu_2$, and $\sigma$) by maximizing the likelihood function. More details can be found in Hamilton (1990)~\cite{90ham}.


There is a vast literature on Bayesian approach to this problem, see for example, Chib (1998)~\cite{chib} and subsequent related publications. An interesting application is given in Pastor and Stambaugh (2001)~\cite{pastor}. Lai and Xing (2013)~\cite{laixing} study a general model, somewhat similar to \eqref{eqn:varautoy}:
	\begin{equation} \label{eqn:arxgarch}
	\text{ARX-GARCH:} \quad y_t= \beta_t' x_t + v_t \sqrt{h_t} \, \epsilon_t,
	\end{equation}
where $h_t \sim \text{GARCH}(p,q)$ and $\beta_t$ and $v_t$ are piecewise linear. Using the weakly returns of S\&P 500 index, AR(1)-GARCH(1,1) (AR for the mean and GARCH for the variance) model is attempted and it is identified that 1998--2003 had higher volatility. This can be seen from Figure~\ref{fig:sp500} that provides the plot of closing price, returns, volatility and volume data. In most of the applications of the above regime-switching models, particularly in finance, the plots of relevant quantities clearly reveal the pattern and these models provide a way to confirm the level and size shifts. From the plots, it is easy to see how the volatility in returns increases during the regime changes. 


	\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{chapters/chapter_advanced/figures/sp500.png}
	\caption{Plot of S\&P 500 Index. \label{fig:sp500}}
	\end{figure}
	

We want to conclude this section with some simple ideas borrowed from the area of statistical process control. Recall that if the market is efficient, the return $r_t= \ln P_t - \ln P_{t-1} \sim \text{ IID}(\mu_t, \sigma^2)$, where $\mu_t=0$. But assume that $\mu_t=c\delta$ when $t\geq t_0$ and zero elsewhere. The cumulative sum chart (CUSUM) to detect the positive and negative mean shift are based on recursive calculations:
	\begin{equation} \label{eqn:cdouble}
	\begin{aligned}
	c_t^+&= \max\big(0, c_{t-1}^+ + (r_t-k)\big) \\
	c_t^-&= \min\big(0, c_{t-1}^- - (r_t-k)\big),
	\end{aligned}
	\end{equation}
where $k= \delta/2$. The chart signals as sown as $c_t^+$ reaches `$h$'---a decision point set by the user. Jiang, Shu and Apley (2008)~\cite{shuap} modify \eqref{eqn:cdouble} with exponentially weighted moving average (EWMA) type of estimates that is adaptive in nature. It can be stated as,
	\begin{equation}\label{eqn:elma}
	s_t^+= \max \left( 0, s_{t-1}^+ + w(\hat{\delta}_t) \left(r_t - \frac{\hat{\delta}_t}{2} \right) \right),
	\end{equation}
where $w(\hat{\delta}_t)$ is a weight function; a simple weight function $w(\hat{\delta}_t)=\hat{\delta}_t$ is suggested. The $\delta_t$ is updated as 
	\begin{equation}\label{eqn:updatedelt}
	\hat{\delta}_t= (1-\lambda)\, \hat{\delta}_{t-1} + \lambda r_t.
	\end{equation}
The adaptive CUSUM chart is effective at detecting a range of mean shift sizes. Similar results for the variance need to be worked out.


In practical applications of these regime-shifting models, it is not easy to discern between a single outlier and persistent level shift. Valid measures that can distinguish these two situations on a timely basis could be very valuable to a trader. We also want to remind the reader that the Markov switching models of changing regimes are indeed latent variable time series models. These models are known as hidden Markov models (HMMs) and represent a special class of dependent mixtures of several processes, these are closely related to state-space models in Section~5.1. An unobservable $m$-state Markov chain determines the state or regime and a state-dependent process of observations. The major difference is that HMMs have discrete states while the Kalman filter based approach refers to latent continuous states. 



% A Model for Volume-Volatility Relationship
\section{A Model for Volume-Volatility Relationship \label{s:model_volvol_rel}} \label{in:volvol1}

While the price information via returns and the volatility can be modeled through methods described earlier, it has become important to incorporate other relevant trading data to augment the trading strategies. To that effect, we want to seek some guidance from economic theory how that information is linked to price movements.


In the market microstructure theory, it is assumed that price movements occur primarily due to the arrival of new information and this information is incorporated efficiently into market price. Other variables such as trading volume, bid-ask spread and market liquidity are observed to be related to the volatility of the returns. Early empirical studies have documented a positive relationship between daily trading volume and volatility (See Clark (1973)~\cite{clark}). Epps and Epps (1976)~\cite{epps} and Tauchen and Pitts (1983)~\cite{tauchenpitts} assume that both the volume and price are driven by the underlying `latent' information ($I$) and provide a theoretical framework to incorporate this information. Assuming that there are fixed number of traders who trade in a day and the number of daily equilibria, $I$ is random because the number of new pieces of information is random, the return and the volume are written as a bivariate normal (continuous) mixture model with the same mixing variable $I$. Conditional on $I$:
	\begin{equation} \label{eqn:2rtVt}
	\begin{aligned}
	 r_t&= \sigma_1 \sqrt{I_t}Z_{1t} \\
	 V_t&= \mu_VI_t + \sigma_2 \sqrt{I_t}Z_{2t}
	 \end{aligned}
	 \end{equation}
where $Z_{1t}$ and $Z_{2t}$ are standard normal variables and $Z_{1t},  Z_{2t}$, and $I_t$ are mutually independent. Thus the volatility-volume relationship,
	\begin{equation} \label{eqn:2covVt}
	\begin{aligned}
	\cov(r_t^2,V_t)& = \Ex(r_{t}^2V_t) - \Ex(r_t^2) \,\Ex(V_t) \\ 
	&= \sigma_1^2 \mu_V \var(I_t) > 0
	\end{aligned}
	\end{equation}
is positive due to the variance in $I_t$. If there is no new information or if there is no variation in the mixing variable, $\var(I_t)= 0$ and thus the relationship vanishes. The theory of arrival rates suggest a
Poisson distribution\label{in:poisson} and based on empirical evidence, the lognormal distribution is taken to be a candidate for distribution of mixing variable, $I_t$. The parameters of the model in \eqref{eqn:2rtVt} are estimated through maximum likelihood. Gallant, Rossi and Tauchen (1992)~\cite{grt} using a semi-parametric estimate of the joint density of $r_t$ and $V_t$ conduct a comprehensive study of NYSE data from 1925 to 1987. The following summarizes their main findings: \twomedskip


\noindent\textbf{Stylized Fact 6 (Volume--Volatility Relationship):} \label{in:style5}

        \begin{itemize}
        \item  There is a positive correlation between conditional volatility and volume.
        \item Large price movements are followed by high volume.
        \item Conditioning on lagged volume substantially attenuates the leverage (which is an asymmetry in the conditional variance of current price change against past price change) effect.
        \item After conditioning on lagged value, there is a positive risk-return relation.
        \end{itemize}


These findings can be easily confirmed using the S\&P 500 data presented in Figure~\ref{fig:sp500}. During the period of regime changes (for example, October 1999--November 2000), we should expect higher volatility and therefore higher correlation with volume. 


Andersen (1996)~\cite{andersen} modifies the model \eqref{eqn:2rtVt} by integrating the microstructure
setting of Glosten and Milgrom (1985)~\cite{glostenmilgrom} with the stochastic volatility, that is built on weaker conditions on the information arrival process. While the first equation in \eqref{eqn:2rtVt} remains the same, the volume $V_t$ has informed and noise components, $V_t = IV_t + NV_t$. Noise trading component, $NV_t$ is taken as a time homogeneous Poisson process, $P_0(m_0)$. Therefore, the systematic variation in trading is mainly due to informed volume. Then $IV_t \;|\; I_t \sim \text{Poisson}(I_t \mu)$ and thus
	\begin{equation} \label{eqn:2VtIt}
	V_t \;|\; I_t \sim \text{Poisson}(m_0 + I_t m_1)
	\end{equation}
It is easy to see $\cov(r_t^2,V_t) = \sigma^2 m_1 \var(I_t) > 0$ under this setting as well. These models clearly indicate that the intraday return volatility and volume processes jointly contain some predictable elements.


There are other studies that focus on the trading volume and returns relationship and we mention only a select few here. Campbell, Grossman, and Wang (1993)~\cite{campbellgross} demonstrate for individual large stocks and stock indices the first-order daily auto-correlation in the returns tend to decline with volume. The authors develop a theoretical model where the economy has two assets, risk-free asset and a stock and there are two types of investors, one with constant risk aversion and the other with risk aversion changing over time. Under this set-up, it is implied that a decline in a stock price on a high-volume day is more likely than a decline on a low-volume day to be associated with an increase in the expected stock return.


Gervais, Kaniel, and Mingelgrin (2001)~\cite{gervais2001high} investigate the role of trading activity in providing information about future prices. It is shown that periods of extremely high (low) volume tend to be followed by positive (negative) excess returns. The formation period for identifying extreme trading volume is a day or a week, but the effect lasts at least twenty days and holds across all stock sizes. To test if this information can be used profitably, the authors construct a trading strategy, by sending buy (sell) limit orders at the existing bid (ask) price, at the end of the formation period. If the orders are not taken, they are converted into market orders at the closing. The strategy is shown to result in profits, in particular with the small-medium firm stocks, after adjusting for the transaction costs. The model used for the empirical framework is the vector autoregressive model discussed in Chapter~\ref{ch:ch_mvts}, with an addition of a set of exogenous control variables:

	\begin{equation} \label{eqn:2Ytphi}
	Y_t = \Phi_0 + \sum_{j=1}^p \Phi_j Y_{t-j} + \sum_{l=1}^L B_l X_{t-l} + \epsilon_t.
	\end{equation}


The components of $Y_t$ include stock or market related variables such as detrended log of stock turnover, the stock return and the value weighted market return. The control variables are market volatility based on the daily return standard deviation and the dispersion which is the cross-sectional standard deviation of the security returns. The impulse response functions are used to aggregate the overall relationship among the endogenous variables. Statman, Thorley, and Vorkink (2006)~\cite{statman2006investor} show that the trading volume is dependent on past returns over many months and it is argued that this may be due to the overconfidence of the investors.


These and other studies generally confirm the information content of the volume and turnover of stocks traded in the low frequency context; strategies to exploit these, especially in a high frequency context, will be mentioned in Chapter~\ref{ch:stat_ts}. We will discuss how intra-day flow of volume can be related to price changes. 


In algorithmic trading a key ingredient of many strategies is forecast of intra-day volume. Typically a parent order is split into several child orders and the timing of the submission of child orders could depend on the volume forecast for an interval of time that is considered for trading and when child orders are sent out. Brownlees, Cipollini and Gallo (2011)~\cite{brownless} provide prediction model for intra-day volume. This will be described in detail in the next chapter.


The model in \eqref{eqn:2rtVt} is extended to cover multiple stocks in He and Velu (2014)~\cite{hevelu}. The role of common cross-equity variation in trade related variables is of interest in financial economics. The factors that influence the prices, order flows and liquidity are most likely to be common among equities that are exposed to the same risk factors. Exploring commonality is useful for institutional trading such as portfolio rebalancing. The mixture distribution in \eqref{eqn:2rtVt} is developed by assuming that factor structures for returns and trading volume stem from the same valuation fundamentals and depend on a common latent information flow. \label{in:volvol2} \label{in:style6}



% Models for Point Processes
\section{Models for Point Processes} \label{in:point1}

In many fields of study, observations occur in a continuum, space or time in the form of point events. The continuum can be multi-dimensional, but our focus is on the one-dimensional time scale with points distributed irregularly along the time scale. The main interest lies in estimating the mean rate of occurrence of events or more broadly on the patterns of occurrences. There are excellent monographs on this topic: Cox and Lewis (1966)~\cite{cox1966}, Daley and Vere-Jones (2003)~\cite{daley2003}, etc. But the interest in financial applications was revived by the seminal paper by Engle and Russell (1998)~\cite{engle1998}. Financial market microstructure theories as discussed in Kyle (1985)~\cite{kyle1985}, Admati and Pfleiderer (1988)~\cite{admati1988theory} and Easley and O' Hara (1992)~\cite{easley1992} suggest that the frequency and the timing of trade-related activities, that include posting, canceling and executing an order, carry information about the state of the market. The transactions generally tend to cluster during certain times of the day and the change in the mean rate of occurrences may suggest a new information flow about a stock.


To illustrate, recall Figure~\ref{fig:tradeactline}, on trading activities in Section 3.1. If we denote the observed intervals between successive activities as durations by $d_1, d_2, \ldots,d_r$, the time of occurrences are obtained by forming the cumulative sums of the $d$'s, $t_1= d_1, t_2= t_1 + d_2, \ldots, t_r=t_{r-1} + d_r$. Cox and Lewis (1966)~\cite{cox1966} suggest two ways to present this type of data graphically. One method is based on cumulative numbers of events that have occurred at or before `$t$' against `$t$'. The slope of the line between any two points is the average number of events per unit for that period. One way to standardize the plot would be to approximate the graph by a line, `$a\,t$' when `$a$' is the slope of the graph indicating the average rate of occurrence for the entire duration. The second method calls for dividing the time scale into equally spaced time intervals and count the number of events in each interval; this also can be alternatively studied by fixing a certain number of events and count on the time it takes for this number to occur. In the context of stock data, this could mean simply recording not when the trade occurs but when the price changes. The advantage of the second plot is that the local fluctuations are readily observed and the advantage of the first plot is that it enables us to see systematic changes in the rate of occurrence.
 
 	\begin{figure}[!ht]
	\centering	
	\includegraphics[width=\textwidth]{chapters/chapter_advanced/figures/survivor.png}
	\caption{Empirical Survivor Function and Serial Correlation plots.\label{fig:survivor}}
	\end{figure}
	
	\begin{figure}[!ht]
	\centering
	 \includegraphics[width=\textwidth]{chapters/chapter_advanced/figures/afc_duration.png}
	\caption{Autocorrelation Function for duration (with 5\% significance limits for the autocorrelations). \label{fig:duration}}
	\end{figure}
 
 The baseline distribution for the durations is exponential resulting from Poisson arrivals for a specified interval that assume a constant rate. It is suggested to plot $\log(1 - \frac{i}{n_0+1})$ against $d_{(i)}$, where $d_{(i)}$ results from the ordered durations, $d_{(1)} \leq d_{(2)} \leq \cdots \leq d_{(n_0)}$ calculated between successive $n_0 + 1$ occurrences. Departures from linearity would indicate that the exponential distribution may not hold. In addition it is also suggested to plot $d_{i+1}$ against $d_i$ to check on the dependance of durations. Cox and Lewis (1966)~\cite[p.14]{cox1966} state that, ``It is not clear how correlation in such data should be measured and tested; in particular it is not obvious that the ordinary correlation coefficient will be the most suitable measure with such data.'' Engle and Russell (1998)~\cite{engle1998} show how the dependence in the duration data can be explicitly modeled in the context of high frequency transaction data. We illustrate this with the trading data for Microsoft (MSFT) for the month of January in the year 2013. The graph (Figure~\ref{fig:survivor}) clearly indicates that the durations do not follow an exponential distribution and do exhibit some dependence. This is confirmed by the autocorrelation function for durations (Figure~\ref{fig:duration}).


Cox and Lewis (1966)~\cite{cox1966} also observe a few issues that can arise with the point-process type data. It is possible that two or more events recorded are happening at the same time. This may be due to latency issues but if they occur due to genuine coincidences it is better to analyze the number of events attached to each occurrence time as a variate separately. Further complications arise from the fact there may be events, such as price and volume that may be interdependent and events that occur with related series that may be useful for modeling and predicting the future occurrence times.



% Stylized Models for High Frequency Financial Data
\subsection{Stylized Models for High Frequency Financial Data} \label{in:style7}

There has been a great deal of interest in studying market microstructure to better understand the trading mechanisms and the process of price formation. With the availability of Trades and Quotes (TAQ)\label{in:taq2} data that contains all equity transactions on major exchanges, it is possible to better model the key elements of market microstructure. The extensive details from the order books over multiple exchanges provide massive amount of data, the analysis of which will be taken up in later chapters as well. Here we want to present some unique characteristics of high frequency data and present most commonly used models in practice. To reiterate, the transactions (trades, quotes, bids, etc) may occur at any point in time (Figure~\ref{fig:exchhours}) during the exchange hours as given below.

	\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}
	\draw[very thick] (0,0) -- (10,0);
	
	\draw[very thick] (0,-0.3) -- (0,0.3);
	\draw[very thick] (10,-0.3) -- (10,0.3);
	
	\draw[very thick] (1,-0.3) -- (1,0.3);
	\draw[very thick] (2,-0.3) -- (2,0.3);
	\draw[very thick] (5,-0.3) -- (5,0.3);
	
	\node at (1,-0.7) {$t_1$};
	\node at (2,-0.7) {$t_2$};
	\node at (5,-0.7) {$t_3$};
	
	\node at (1,-1.2) {$y_1$};
	\node at (2,-1.2) {$y_2$};
	\node at (5,-1.2) {$y_3$};
	
	\node at (0,-0.7) {9:30~a.m.};
	\node at (10,-0.7) {4:00~p.m.};
	
	\foreach \i in {1,...,5}
	{
	\draw[fill=black] (2.5+\i/3,-0.95) circle (0.2mm);
	};
		
	\end{tikzpicture}
	\caption{Exchange hours.\label{fig:exchhours}}
	\end{figure}

High frequency data refer to the `tick' `$t_i$' data which contains, in addition to the exact time of even occurrence, `$y_i$' called marks that may refer to all other elements of the limit order book such as traded price, quote, book imbalance, etc. that may be associated with `$t_i$.' The traditional time series methods that we discussed in the context of low frequency data analysis are not applicable here as the ticks can occur at any point in time when the exchange is open. Standard econometric techniques are for data in discrete time intervals, but aggregating\label{in:agg} high frequency data to some fixed time interval would not capture the advantages of having access to detailed time-stamped transaction data. Even for some heavily traded stocks, if the aggregation intervals are chosen to be short, there may be many intervals with no data and if the intervals are long, the microstructure features will be lost. Also certain key features such as the imbalance between bid side and ask side of the limit order book and when and how the market orders cross the spread are not easy to aggregate in a fixed time interval, however short or long it may be. Moreover, the timings of transactions provide valuable information for trading. Some noted features of high frequency data are:


\begin{itemize}
\item \textbf{Nonsynchronicity:} Different stocks have different trade intensities and even for a single stock, the intensity can vary during the course of a day. For the aggregated low frequency (daily) data, thus, we cannot assume that daily returns occur in equally-spaced time series.

\item \textbf{Multiple transactions with the same time stamp:} It is possible that in periods of heavy trading especially around the opening and closing times of the exchange, each tick may contain multiple transactions. For the analysis of this type of occurrence, simple aggregate summary measures to more elaborate measures such as the variation in prices in addition to average prices are suggested.

\item \textbf{Multiple Exchanges:} In the US market there are currently thirteen lit exchanges; due to latency issues there could be time delays in recording the submission times of orders that get dispatched at the same time. Also with Reg NMS (NBBO) on getting best price anywhere in the market, an aggregated view of the market given a fixed time interval can miss capturing the other dependencies over exchanges. 

\item \textbf{Intra-day Periodicity:} Generally, it is observed for stocks, transaction activities are higher near the open and the close than in the middle of the day. Thus volatility is higher, immediately after the opening and before the closing of the market resulting in U-shape pattern of activity and volume.

\item \textbf{Temporal Dependence:} High frequency data generally exhibit some dependence. The dependence is due to\dots
        \begin{itemize}
        \item price discovery
        \item bid-ask bounce
        \item execution clustering of orders
        \end{itemize}

\item Volatility, volume are higher near the open and close, spreads are larger near the open and narrower near the close.

\item Time between trades, shorter near open and close.
\end{itemize}


	\begin{figure}[!ht]
	\centering
	\pgfplotsset{every axis/.style={scale only axis}}
	\begin{tikzpicture}
	\begin{axis}[
	hide axis,
	height=6cm,
	width=12cm,
	xmin=-2,xmax=6,
	ymin=-2,ymax=10
	]
	\addplot[domain=0:10,very thick,->] ({0},{x});
	\addplot[domain=0:4,very thick,->] ({x},{0});
	\addplot[domain= 0.5:3.5,smooth,ultra thick] {-1.6*(x-2)^2+9.5};
	\addplot[domain= 0.5:3.5,dashed,smooth,ultra thick] {(x-2)^2+2};
	\node at (2,-1) {\bfseries Time of Day};
	\node at (-0.7,5.9) {\bfseries Cents/};
	\node at (-0.7,4.9) {\bfseries Seconds};
	\node at (3.6,9) {\bfseries Duration};
	\node at (4.1,2) {\bfseries Standard deviation of};
	\node at (4.2,0.9) {\bfseries mid-quote price change};
	\end{axis}
	\end{tikzpicture}
	\caption{Typical Diurnal patterns.\label{fig:diurnal}}
	\end{figure}


Under aggregation with low frequency data generally the dependence tends to decrease. \twomedskip


\noindent\textbf{Stylized Fact 7 (Continued from Chapter~\ref{ch:ch_uvts}): Negative autocorrelation in returns:} Non-synchronous trading and bid-ask bounce both can introduce negative lag-1 autocorrelation in the returns. See Roll (1984)~\cite{roll1984} and Lo and Mackinlay (1990)~\cite{lo1990}.\twomedskip


We want to briefly again discuss the Roll model that was mentioned in Chapter~\ref{chap:ch_trading_fund}; on a typical day of trading, stocks exhibit price changes numerous times. These changes merely represent market frictions between the demand and supply sides and are not necessarily due to arrival of new information about the stock. Assume that the true price (unobservable) follows a random walk model, Example~\ref{ex:driftwalk} Chapter 3, as
	\begin{equation} \label{eqn:2plowstar}
	p_t^*=p_{t-1}^* + \epsilon_t 
	\end{equation}
and the observed price,
	\begin{equation} \label{eqn:2lowpstar}
	p_t = p_t^* + u_t
	\end{equation}
with `$\epsilon_t$' denoting the true information about the stock and `$u_t$' denoting the market friction. But the observed return,
	\begin{equation} \label{eqn:2firstrtlow}
	r_t=p_t-p_{t-1} = \epsilon_t + u_t - u_{t-1}
	\end{equation}
inducing the negative autocorrelation of lag 1 in the return due to the carry over term, `$u_{t-1}$'. The term on the right-hand side because of zero autocorrelation beyond lag-1 can be written as MA(1) model:
	\begin{equation} \label{eqn:2lowrt}
	r_t=a_t - \theta a_{t-1}.
	\end{equation}
The value of `$\theta$' generally tends to be small, but nevertheless significant. \twomedskip


\noindent \emph{Commonly used duration models:} The timing aspect of transaction was discussed in Easley and O'Hara (1992)~\cite{easley1992} and a rigorous set of tools was developed by Engle and Russell (1998)~\cite{engle1998}, as an alternative to fixed interval analysis. Treating the ticks as random variables that follow a point process, the intensity of transactions in an interval of length, $\Delta$, is defined as
	\begin{equation} \label{eqn:2lambda}
	\lambda(t)= \lim_{\Delta \to 0} \frac{\Ex[N(t+\Delta) - N(t) \;|\; F_t]}{\Delta},
	\end{equation}
where $N(t)$ denotes the number of ticks until `$t$' and $F_t$ is the information available until that time. The commonly used model for such arrival rates is Poisson that implies that the time between two successive ticks is independent of pairwise ticks and follows an exponential distribution. But the duration data, $d_i = t_i - t_{i-1}$ exhibit some dependence, thus implying that time moves faster sometimes, faster than the clock time. The intensity rate, $\lambda(t)$ is not a constant, which is a typical characteristic of the exponential distribution. We briefly outline how the durations are modeled. For detailed discussion, see Tsay (2010)~\cite{tsay} or Lai and Xing (2008)~\cite[Section 11.2]{lai1}. The autoregressive conditional duration (ACD) model\label{in:acd} is defined as follows:
	\begin{flalign}\label{eqn:2dipsi}
	&& d_i&= t_i - t_{i-1} = \psi_i \varepsilon_i, && \notag \\ 
	\text{and} && \phantom{x} & \phantom{x} &&  \\
	&& \psi_i&= \alpha + \sum_{j=1}^p \alpha_j d_{t-j} + \sum_{v=1}^q \beta_v \psi_{i-v}, && \notag
	\end{flalign}
where $\varepsilon_i$ are i.i.d. with $\Ex(\varepsilon_i) = 1$; If $\varepsilon_t's$ are assumed to follow exponential the model \eqref{eqn:2dipsi} is called E(xpotential)ACD. Two alternative models WACD and GGACD are based on the following specification for the errors:
	\begin{equation} \label{eqn:wei_gam}
	\begin{aligned}
	&\text{Weibull: } h(x)= \frac{\alpha}{\beta^{\alpha}} x^{\alpha - 1} \exp\left\{ -(\frac{x}{\beta})^{\alpha} \right\} \\
	&\text{Generalized Gamma: } y= \lambda \frac{x}{\beta},
	\end{aligned}
	\end{equation}
where $\lambda = \frac{\sqrt{K}}{\sqrt{K + \frac{1}{\alpha}}}$. The hazard function of Generalized Gamma is quite flexible and may fit better various patterns. The stationary conditions would require that the roots of $\alpha(B) = 1 - \sum_{j=1}^q (\alpha_j + \beta_j)\, B^j$ are outside the unit circle when $g= \max(p,q)$ and $B$ is the back-shift operator. It is clear that the model \eqref{eqn:2dipsi} is similar to GARCH model given in \eqref{eqn:2secondht}. The estimation is typically done through conditional maximum likelihood and all inferences are asymptotic.


An alternative more direct ARMA type model for log durations is suggested by Ghysels, Gourieroux and Jasiak (2004)~\cite{jasiak}:
        \begin{equation} \label{eqn:21lnd}
        	\ln{(d_i)} = \omega + \sum_{j=1}^p \alpha_j \ln{(d_{t-j})} + \sum_{j=1}^v \beta_j \varepsilon_{i-j} + \varepsilon_i
        \end{equation}
and $\varepsilon_i = \sqrt{h_i^v} u_i, h_i^v = \omega^v + \sum_{j=1}^{p^v} \alpha_j^v \varepsilon_{i-j}^2 + \sum_{j=1}^{q^v} \beta_j^v h_{i-j}^v$, where $u_i \sim N(0,1)$ i.i.d.; here the conditional mean and variance of durations are assumed to be separable. The duration volatility that is modeled using the second equation in \eqref{eqn:2dipsi} is interpreted as `liquidity' risk. \twomedskip


\noindent \emph{Other Duration Related Models:} So far the discussion has been around modeling the durations, but the information set, $F_t$ has information on `marks', the information associated with the past ticks, such as the number of units transacted, price etc. McCulloch and Tsay (2001)~\cite{mccullochtsay07} suggest the following model:
	\begin{equation} \label{eqn:2lndi}
	\ln{d_i} = \beta_0 + \beta_1 \ln{(d_{t-1})} + \beta_2s_{i-1} + \sigma\varepsilon_i,
	\end{equation}
where $s_i$ is the size of the $i$th price change measured in ticks; other relevant variables can be easily added to this model. \label{in:style8}



% Models for Multiple Assets: high frequency Context
\subsection{Models for Multiple Assets: High Frequency Context}


There is a considerable interest in extending the univariate duration models to multiple stocks or to multiple types of arrival processes. Because investment managers generally consider a portfolio of stocks rather than a single one, it is important for them to follow the transaction processes of several stocks simultaneously. Because of non-synchronous trading relating two stocks with different trading intensities on a common time scale is somewhat difficult. Engle and Lunde (2003)~\cite{englelunde} establish the following model for a bivariate point process for trades and quotes:
	\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}
	\draw[very thick] (0,0) -- (10,0);
	\draw[very thick] (0,-2) -- (10,-2);
	
	\node at (-0.8,0) {\textbf{Trade}};
	\draw[very thick] (0,-0.3) -- (0,0.3);
	\draw[very thick] (1,-0.3) -- (1,0.3);
	\draw[very thick] (2,-0.3) -- (2,0.3);
	\draw[very thick] (5,-0.3) -- (5,0.3);
	\draw[very thick] (10,-0.3) -- (10,0.3);
	\node at (0,-0.7) {$0$};
	\node at (1,-0.7) {$t_1$};
	\node at (2,-0.7) {$t_2$};
	\node at (5,-0.7) {$t_i$};
	\node at (10,-0.7) {$T$};

	\foreach \i in {1,...,5}
	{
	\draw[fill=black] (2.5+\i/3,-0.6) circle (0.2mm);
	};
	
	\node at (-0.8,-2) {\textbf{Quote}};
	\draw[very thick] (0,-2.3) -- (0,-1.7);
	\draw[very thick] (1.5,-2.3) -- (1.5,-1.7);
	\draw[very thick] (5,-2.3) -- (5,-1.7);
	\draw[very thick] (10,-2.3) -- (10,-1.7);
	\node at (0,-2.7) {$0$};
	\node at (1.5,-2.7) {$t_1^{\text{fw}}$};
	\node at (5,-2.7) {$t_i^{\text{fw}}$};
	\node at (10,-2.7) {$T$};
		
	\end{tikzpicture}
	\caption{Trades and Quotes.\label{fig:tradeactdoubline}}
	\end{figure}
Define $x_i = t_i - t_{i-1}$ as trade duration and $y_i = t_i^{f_w} - t_{i-1}^{f_w}$ as the forward quote duration; our goal is to model $(x_i,y_i)$ as a bivariate duration process but the underlying timings, as can be noted, are not synchronized. Thus, if $y_i > x_i$, define
	\begin{equation} \label{eqn:anothertildey}
	\widetilde{y}_i = (1 - d_i)\, y_i + d_ix_i,
	\end{equation}
where $d_i$ is indicator variable, $d_i = I_{\{y_i>x_i\}}$. Model $(x_i,\widetilde{y}_i)$ as censored process. The equation \eqref{eqn:anothertildey} provides a mechanism in a way to align the two processes. The joint distribution is given by
	\begin{equation} \label{eqn:2pxiyi}
	p(x_i,\widetilde{y}_i \;|\; F_{i-1}, \omega) = g(x_i \;|\; F_{i-1}, \omega_1)\, f(\widetilde{y}_i \;|\; x_i, F_{i-1}, \omega_2).
	\end{equation}
The first term is the trade density and the second term is the quote density. The $\psi_i$ terms as in \eqref{eqn:2dipsi} are modeled as EACD, $\psi_i= f(\psi_{i-1}, x_{i-1}, z_{i-1})$ and quote density is also modeled as EACD after adjusting for censoring:
	\begin{equation} \label{eqn:2fdot}
	f(y_i \;|\; \cdot) = [h(y_i \;|\; \cdot)]^{1 - d_i} \times S(x_i \;|\; \cdot)^{d_i}.
	\end{equation}
The estimation of the parameters is done through quasi maximum likelihood. Using data for eight stocks, Engle and Lund (2003)~\cite{englelunde} observe that high trade arrival rates, large volume per trade, wide bid-ask spreads, all predict more rapid price revisions. The model given in Engle and Lund (2003)~\cite{englelunde} can be conceptually applied to any two point processes. Extending this concept to more than two processes could be of interest to traders who may want to monitor multiple trading exchanges simultaneously, but modeling this type of data is quite tedious. \label{in:point2}


% Analysis of Time Aggregated Data
\section{Analysis of Time Aggregated Data \label{s:analysis_tad}}

The commonly available data from public sources such as Yahoo Finance provide select price information, High, Low, Open and Close prices along with the total transaction volume in a trading day. The usual estimate of volatility is based on closing prices, but having this additional price (bars) data can help to get a better estimate of volatility. As shown in the last section, using high frequency data leads to estimators that need to be corrected for market frictions. But using daily price bars data may not be helpful to make entry/exit decisions as they are to be made in real time; so as a compromise, many traders construct price bars data from high frequency point process data for a shorter time intervals such as 5--30~minutes. This has the effect of smoothing the noise arising from market frictions but also results in data in discrete time units allowing the traditional discrete time series methods discussed in Chapter 3 and Chapter 4 to be effectively employed. 



% Realized Volatility and Econometric Models
\subsection{Realized Volatility and Econometric Models} \label{in:disc_time_unit2}

Estimating volatility using high frequency data has received a great deal of attention as intra-day strategies became more prevalent. In the low frequency (daily) level some estimators based on select prices were presented in Chapter~3. Intra-day dynamics of volatility is of interest to traders and is usually not fully captured by the stock price indicators sampled during the day. If higher frequency data are used to estimate volatility of lower frequency data, it is important to know the model for the return at the lower frequency. To illustrate this, we consider data observed at two time scales; although they are somewhat at low frequency level, the concept can be easily extended to the high frequency context. If $r_{t,i}$ is the $i$th day return in $t$th month, the $t$th month return assuming there `n' trading days, $r_t^m = \sum_{i=1}^n r_{t,i}$. Note that $\sigma_m^2 = \var(r_t^m \;|\; F_{t-1}) = \sum_{i=1}^n \var(r_{t,i} \;|\; F_{t-1}) + 2 \sum_{i<j} \cov(r_{t,i}, r_{t,j} \;|\; F_{t-1})$. If $r_{t,i}$ is white noise sequence, then
	\begin{equation}\label{eqn:2hatsigmasq}
	\hat{\sigma}_m^2 = \frac{n}{n-1} \sum_{i=1}^n (r_{t,i} - \overline{r}_t)^2.
	\end{equation}
But we had observed that in the high frequency data, returns do exhibit some serial correlation and so adjusting \eqref{eqn:2hatsigmasq} for serial correlation is important to get a more accurate estimate of volatility. A simpler estimate of $\sigma_m^2$ is the so-called realized volatility ($\text{RV}_t$)
	\begin{equation} \label{eqn:2RV}
	\text{RV}_t = \sum_{i=1}^n r_{t,i}^2.
	\end{equation}
But in the estimation of intra-day volatility, the number of sampling intervals, `$n$' and hence, `$\Delta$', the interval size can affect the estimate. If $\Delta \rightarrow 0$, the value $\text{RV}_t$ in the $\Delta$-interval goes to infinity. Thus the optimal choice of `$\Delta$' is crucial and is reviewed below, for a judicious choice.


Bandi and Russell (2006)~\cite{bandi} argue that the observed price is the sum of efficient price and a friction price (see \eqref{eqn:2lowpstar}) that may be induced by bid-ask bounce, price discreteness etc. Thus the observed variance such as \eqref{eqn:2hatsigmasq} is the sum of variance of the efficient returns and the variance of microstructure noise. As they indicate, both are useful: ``The variance of the efficient return process is a crucial ingredient in the practice and theory of asset valuation and risk management. The variance of microstructure noise components reflects the market structure and the price setting behavior of market participants and thereby contains information about the market fine-grain dynamics.'' Both components can be estimated using high frequency data but sampled at different frequencies. Data sampled at low frequency are used to estimate the efficient return variable and the data of high frequency are used to estimate the microstructure noise variance. Additional references on this topic include, A{\"\i}t-Sahalia, Mykland and Zhang (2005)~\cite{ait2005often} and Barndoff-Nielsen and Shepard (2002)~\cite{barndorff2002econometric}.


On any given day, assume that the observed price, at time `$i$' is as in (\ref{eqn:2lowpstar})
	\begin{equation} \label{eqn:2pi2}
	p_i = p_i^* + u_i, \quad i= 1, 2, \ldots, n,
	\end{equation}
where $p_i^*$ is the efficient price and $u_i$ is the of microstructure noise. Dividing a trading day into $M$ sub-periods, where $\delta= 1/M$ is the size of the sub-period, write the observed returns as
	\begin{equation} \label{eqn:2rji2}
	r_{j,i} = r_{j,i}^* + \varepsilon_{j,i}, \quad j= 1,2,\ldots, M.
	\end{equation}
Assume that the frictions, $u$'s are i.i.d. mean zero and variance $\sigma_{u}^2$. Because $\varepsilon_{j,i}= u_{(i-1) + j \delta} - u_{(i-1) + (j-1)\delta}$, $\var(\varepsilon_{j,i})= 2 \sigma_{\eta}^2$. It has been noted already that the return series exhibit negative first order autocovariance and $\varepsilon$'s can be taken to represent a variable, $\eta$, with MA(1) structure. The returns are computed at higher frequency at which the new information arrives. Thus $\sum_{j=1}^M r_{j,i}^2/M$ can be used to estimate noise variance. 


To motivate the optimal choice, we consider two possible situations. If there is no microstructure noise, the usual estimate of variance, $\hat{\sigma}^2= \frac{1}{T} \sum_{i=1}^n r_i^2$ has the asymptotic variance `$2\sigma^4 \delta$' which suggests selecting `$\delta$' as small as possible. The implication of the presence of microstructure noise is that returns are likely to follow a MA(1) process with a negative autocorrelation and the proportion of total variance due to market microstructure noise is, $\Pi= \frac{2\sigma_\eta^2}{\sigma^2\delta + 2\sigma_\eta^2}$ and as `$\delta$' gets smaller, this proportion gets closer to unity. This indicates that the optimal sampling frequency should be finite when noise is present.


For the estimation of efficient price variance, we need to consider an  optimal division of a day into `$M$' number of subperiods. The optimal sampling frequency over `$n$' days is chosen as $\delta^* = 1/M^*$ with $M^*= (\hat{Q}_i/\alpha)^{1/3}$, where $\hat{\alpha} = \sum_{i=1}^n \sum_{j=1}^M \overline{r}_{j,i}^2 / (nM)$ and $\hat{Q}_i = \frac{M}{3} \sum_{j=1}^M \hat{r}_{j,i}^4$. Based on the analysis of a sample of S\&P 100 stocks' mid quotes for the month of February 2002, Bandi and Russell (2006)~\cite{bandi} conclude that 5-min frequency is optimal and the 15-min interval may provide the variance estimates `excessively volatile'. The economic benefit of optimal sampling is that high frequency traders can employ a realistic variance forecasts in formulating the entry and exit strategies, by separating the variance due to market frictions. 



%% Analysis of Time Aggregated Data
%\section{Analysis of Time Aggregated Data \label{s:analysis_tad}}
%
%The commonly available data from public sources such as Yahoo Finance provide select price information, High, Low, Open and Close prices along with the total transaction volume in a trading day. The usual estimate of volatility is based on closing prices, but having this additional price (bars) data can help to get a better estimate of volatility. As shown in the last section, using high frequency data leads to estimators that need to be corrected for market frictions. But using daily price bars data may not be helpful to make entry/exit decisions as they are to be made in real time; so as a compromise, many traders construct price bars data from high frequency point process data for a shorter time intervals such as 5--30~minutes. This has the effect of smoothing the noise arising from market frictions but also results in data in discrete time units allowing the traditional discrete time series methods discussed in Chapter 3 and Chapter 4 to be effectively employed. 

% Volatility and Price Bar Data
\subsection{Volatility and Price Bar Data}

The price as assumed earlier is to follow a random walk model. Therefore, price changes (thus returns) over a time interval are distributed with mean zero and variance that is proportional to the length of the interval. Assuming that the prices follow continuous sample paths although the trading is closed for a certain duration and when trading is open, the actual transactions occur at discrete points in time. Treating the trading day as represented in a unit interval $[0,1]$ with $[0,f]$ representing the `market close' time and $[f,1]$ as `open' time, it is shown that an estimator of volatility
	\begin{equation} \label{eqn:estvol}
	\hat{\sigma}_1^2= \dfrac{(O_1 - C_0)^2}{2f} + \dfrac{(C_1 - O_1)^2}{2(1-f)},
	\end{equation}
has efficiency two compared to the usual estimator, $\hat{\sigma}^2_0=(C_1 - C_0)^2$ based on the closing prices of two successive trading days. This suggests clearly the inclusion of additional data point, such as opening price is quite informative. Garman and Klass~\cite{klass1980} suggest these and other estimators which are superior to the classical estimator of volatility $\hat{\sigma}^2_0$. It is argued that the high and low prices contain information regarding the volatility during the trading period and a composite estimator that is proposed,
	\begin{equation} \label{eqn:compositeest}
	\hat{\sigma}_2^2= a\,\dfrac{(O_1 - C_0)^2}{f} + (1-a)\, \dfrac{(H_1 - L_1)^2}{(1-f) \ln 2}
	\end{equation}
with optimal choice of `$a$'$=0.17$ yields even higher efficiency.


The estimators like the above assume that either the price process has no drift or no price jumps between the previous day's closing price and current day's opening price. The former assumption leads to overestimating the volatility and the later to underestimating volatility. Yang and Zhang~\cite{yangzhang2000} provide an estimator based on several periods of low, high, open and close prices that is independent of the drifts and the jumps. Although several estimators are discussed in this paper, we mention only two of them that have high efficiency. Denoting
	\[
	\begin{aligned}
	o&= \ln O_1 - \ln C_0, \text{ the normalized open}; \\
	u&= \ln H_1 - \ln O_1, \text{ the normalized high}; \\
	d&= \ln L_1 - \ln O_1, \text{ the normalized low}; \\
	c&= \ln C_1 - \ln O_1,  \text{ the normalized close},
	\end{aligned}
	\]
the classical estimator based on $n$-period historical data is
	\begin{equation} \label{eqn:nperioddata}
	V_{cc}= \dfrac{1}{n-1} \cdot \sum_{i=1}^n [(o_i + c_i) - (\overline{o} + \overline{c})]^2
	\end{equation}
and the estimator given in Rogers and Satchell (1991)~\cite{rogerssatchell1991}
	\begin{equation} \label{eqn:rogerssatchell}
	V_{\text{RS}}= \dfrac{1}{n} \cdot \sum_{i=1}^n [u_i(u_i - c_i) + d_i(d_i-c_i)]
	\end{equation}
The estimator suggested in Yang and Zhang (2000)~\cite{yangzhang2000} takes the form,
	\begin{equation} \label{eqn:yang2000}
	V_{\text{YZ}}= V_0 + k V_c + (1-k)V_{\text{RS}}
	\end{equation}
where $V_0= \frac{1}{n-1} \sum_{i=1}^n (o_i - \overline{o})^2$ and $V_c= \frac{1}{n-1} \sum_{i=1}^n (c_i - \overline{c})^2$, with the optimal choice of $k= 0.34/(1.34 + (n+1)(n-1))$. \twomedskip


\noindent\textbf{Applications:} It is well-known the standard stochastic volatility models used in finance are difficult to estimate due to their non-Gaussian nature. The proxies, the absolute or squared returns, because of the measurement errors tend to be somewhat inefficient. Alizadeh, Brandt and Diebold (2002)~\cite{diebold} argue that range as a proxy for volatility is superior as it is relatively free of the two problems associated with other measures. In the estimator \eqref{eqn:compositeest}, the second term which is essentially range-based is bound to be always positive. It can also be shown that the standard deviation of the log range is nearly one-fourth the standard deviation of the log absolute return. The range is also more robust to microstructure effects mostly due to bid-ask bounce compared to popular realized volatility measure \eqref{eqn:2RV}. Martens and Dijk (2007)~\cite{dijk} define realized range over an interval of time as,
	\begin{equation} \label{eqn:rrtdelta}
	\text{RR}_t^\Delta= \dfrac{1}{4\ln 2} \cdot \sum_{i=1}^I (\ln H_{t,i} - \ln L_{t,i})^2,
	\end{equation}
where `$\Delta$' is intra-day interval. This estimator is biased and the bias is corrected by scaling $\text{RR}_t^\Delta$ with the ratio of average levels of the daily range and the realized range over the previous `$q$' days. The scaled realized range,
	\begin{equation} \label{eqn:rrstdelta}
	\text{RR}_{s,t}^\Delta= \left( \dfrac{\sum_{l=1}^q \text{RR}_{t-l}}{\sum_{l=1}^q \text{RR}_{t-l}^\Delta}  \right) \text{RR}_t^\Delta
	\end{equation}
is known to estimate the volatility more accurately. 


The range based statistics can also be used to estimate bid-ask spreads. Recall from \eqref{eqn:2lowpstar}, the Roll model assumes that the observed price is the sum of the true price and the error, that reflects the market friction. The friction can be measured by bid-ask spread which is a measure of market liquidity. Trading decisions are made by closely monitoring this measure that varies during a trading day. Corwin and Schultz (2012)~\cite{schultz12} show how this can be estimated using low-frequency range-based statistics. The reasoning goes as follows: As noted in \eqref{eqn:2firstrtlow}, the return is the sum of information and the friction---sources of volatility and the bid-ask spread. The volatility increases with the length of the trading period but not the bid-ask spread. Thus the price range over a two-day period reflects two days' volatility and one spread but the sum of two consecutive days price ranges reflect two days' volatility and twice the spread. This reasoning is used to separate the two effects.


Assume that the true or actual price follows a diffusion process. Actual price differs from the true price by $s/2$, where $s$ is the spread that remains constant over two consecutive days. Further, it is assumed that daily high price is a buyer-initiated trade while the daily low price is a seller initiated trade. Write,
	\begin{equation} \label{eqn:sellertrade}
	\begin{aligned}
	\left[ \ln \left( \dfrac{H_t^0}{L_t^0} \right) \right]^2&= \left[ \ln \left( \dfrac{H_t^A(1+s/2)}{L_t^A(1-s/2)} \right) \right]^2 \\
	&=\left[ \ln \left( \dfrac{H_t^A}{L_t^A} \right) \right]^2 + 2 \alpha \ln\left( \dfrac{H_t^A}{L_t^A} \right) + \alpha^2,
	\end{aligned}
	\end{equation}
where $\alpha=\ln\left( \frac{1+s/2}{1-s/2} \right)$ is taken to be fixed. Taking expectations on both sides, we have
	\begin{equation} \label{eqn:expsides}
	E\left[ \ln^2 \left( \dfrac{H_t^0}{L_t^0} \right) \right]= K_1\sigma^2 + 2\alpha K_2 \sigma + \alpha^2,
	\end{equation}
where $K_1= 4\ln 2= E\left( \ln^2 \left( \dfrac{H_t^A}{L_t^A} \right) \right)$ and $K_2= \sqrt{\frac{8}{\pi}}= E\left( \ln\left(\frac{H_t^A}{L_t^A} \right) \right)$. Taking expectations over two days, if we let $\beta= E\left[ \sum_{j=0}^1 \ln^2 \left( \frac{H_{t+j}^0}{L_{t+j}^0} \right) \right]$ results in 
	\begin{equation} \label{eqn:betaresult}
	\beta= 2K_1 \sigma^2 + 4K_2 \alpha \sigma + \alpha^2,
	\end{equation}
which has two unknowns `$\alpha$' and `$\sigma$' to solve. If we let $\gamma= \ln^2(H_{t,t+1}^0/L_{t,t+1}^0)$, where the subscripts $(t,t+1)$ indicates that the values are computed over a two day period `$t$' and `$t+1$'. This leads to
	\begin{equation} \label{eqn:resultsin}
	\gamma= 2K_1\sigma^2 + 2\sqrt{2} K_2\alpha \sigma + \alpha^2.
	\end{equation}
Both \eqref{eqn:betaresult} and \eqref{eqn:resultsin} are two quadratic equations in `$\alpha$' and `$\sigma$' that can be iteratively solved. The explicit solutions are given in Corwin and Schultz (2012)~\cite{schultz12}.


Abdi and Ranaldo (2017)~\cite{abdi} provide an estimator that includes the closing price that is easier to calculate and performs well in terms of the criteria considered earlier. Define the mid-range estimate as, $\eta_t=\frac{\ln H_t + \ln L_t}{2}$ and $c_t= \ln C_t$. Then the effective spread can be obtained from,
	\begin{equation}\label{eqn:obtainedfrom}
	s^2=4 E\left[ (c_t-\eta_t) (c_t-\eta_{t+1}) \right].
	\end{equation}
An estimate of `$s$' assuming `$N$' trading days in a month is:
	\begin{equation}\label{eqn:monthlycorrected}
	\hat{s}= \dfrac{1}{N} \sum_{t=1}^N \hat{s}_t,
	\end{equation}
where $\hat{s}_t= \sqrt{ \max\left\{ 4(c_t-\eta_t)(c_t-\eta_{t+1}), 0 \right\} }$. This estimate tends to perform well in terms of both cross-sectional correlates and time series correlations. It also seems to have some predictive power. 


There are two main reasons for the exposition of price bar data to capture the essential aspects of market movement. The price bar data in discrete time intervals is easier to analyze; it can also smooth out the market frictions. By the choice of short time intervals, it can closely approximate the high frequency data. But how the entry/exit decisions are made and their accuracy based on price bar data needs further investigation. 



% Analytics from Machine Learning Literature
\section{Analytics from Machine Learning Literature\label{sec:an_mach_learn}}

The technical trading rules that are to be discussed in Chapter~6 involve a relatively small information set to carry out predictions.\label{in:machine1} It treats each of the `$n$' time series of asset returns as autonomous. As pointed out by Malkiel (2012)~\cite{malkiel}, these rules can be easily implemented by most market participants if such opportunities should emerge, and market efficiency would rule them out as winning strategies. More powerful prediction methods using a very large set of potential predictors and yet capable of avoiding overfitting are needed to take advantage of transient opportunities. We gave an overview of recent advances in high-dimensional regression in Chapter~4. The classification techniques can also be formulated with high-dimensional feature vector. In machine learning and in computer science the focus is to handle rapidly different types of data (including text). The literature in this field is vast and so richly detailed that a single chapter wouldn't do justice to the topic. Consequently, here we will only briefly mention tools that are deemed to be relevant to trading and invite interested readers to further their knowledge with dedicated references such as Goodfellow, Bengio, and Courville (2016)~\cite{goodfellow} and Lopez de Prado (2018)~\cite{LopezDePradoML}.


Machine learning is a still-growing area of computer science that encompasses other well-established areas such as statistics, computational algorithms, control theory, etc.. The focus in machine learning is on developing efficient algorithms for prediction or for classification using large data sets. The efficiency is gauged by predictive validation accuracy. The inferential aspects of statistical theory, such as standard error of the estimates, confidence intervals, etc., are generally not of much concern. Some areas where machine learning methods have led to significant contribution are classification, clustering and multi-dimensional regression. The attractiveness of these methods lies in the fact that they do not need any a priori theory to suggest which relevant variables to consider. Therefore with no prescription of variables, the variable or feature selection becomes an important process in machine learning. The statistical foundations of machine learning methods are also alternatively referred to as statistical learning methods. We provide a brief description of a select few methods in this section. 



% Neural Networks
\subsection{Neural Networks\label{sec:neural_networks}}\label{in:neural_net}

This is one of the main methods of machine learning where a performance of a task is learnt by analyzing training examples that have been hand-labeled in advance. The neural network in its simplest form can be represented as follows:

	\begin{equation} \label{eqn:neural}
	\begin{tikzpicture}[baseline=(current  bounding  box.center)]
	\node(char)[draw,fill=white,shape=rectangle, drop shadow={opacity=.5, xshift=0pt},minimum width=0.7cm, minimum height=0.7cm] at (-4,0) (X) {$\mathbf{X}$};
	\node(char)[draw,fill=white,shape=rounded rectangle, drop shadow={opacity=.5, xshift=0pt},minimum width=0.7cm, minimum height=0.7cm] at (0,0) (Z) {$\mathbf{Z}$};
	\node(char)[draw,fill=white,shape=rectangle, drop shadow={opacity=.5, xshift=0pt},minimum width=0.7cm, minimum height=0.7cm] at (4,0) (Y){$\mathbf{Y}$};
	\draw[->,line width=0.4mm] (X) edge (Z);
	\draw[->,line width=0.4mm] (Z) edge (Y);
	\end{tikzpicture}
	\end{equation}


Here, the square boxes contain `$n$' dimensional input or feature vector, $X$ and `$m$' dimensional output vector, $Y$ and the circular box indicates hidden or unknown layer in-between that connects the input to the output. Structurally, neural networks are a two-stage regression or classification model with intermediary layers, and conceptually the model is similar to the reduced-rank regression model \ref{eqn:ykabxk}. But, $Z$ is generally a non-linear function of $X$. In the ordinary least squares regression model, the goal is to get `$m$' linear combinations of `$X$' that best predicts `$Y$'. Here `$r$' dimensional intermediaries can vary based on the assumption of `hidden' layers, but the `key' is that the output vector, `$Y$', can be a non-linear function of `$X$' via the hidden layers and `$r$' can be larger than `$n$'. In its simplest form the neural network model can be written as follows:
	\begin{equation} \label{eqn:neurallabel}
	\begin{aligned}
	Z_i&= \sigma(\beta_i'X), \quad i=1,2,\ldots,r \\
	Y_j&= g_j(Z), \quad j=1,2,\ldots,m,
	\end{aligned}
	\end{equation}
where the function $\sigma(u)=\frac{1}{1+e^{-u}}$ is the sigmoid function. Note that this is the  function used in logistic regression. In the regression set-up, $g_j(Z)=\alpha_j' Z$, but in the $m$-class classification,
	\begin{equation} \label{eqn:gjzclass}
	g_j(Z)= \dfrac{e^{\alpha_j' Z}}{\displaystyle\sum_{l=1}^m e^{\alpha_j' Z}},
	\end{equation}
called the softmax function, is used. In the set-up given in \eqref{eqn:neural}, we assume only one hidden layer, but in practical applications many layers are assumed which leads to non-uniqueness problems. As shown in Hastie, Tibshirani and Friedman (2009)~\cite{hastibf} the neural network problem is closely related to a non-parametric method called projection pursuit regression. In its simplest form where the information flows in only one direction, it is called a feed-forward neural net, which is commonly used in many applications.


To estimate the `$r \times n$' parameters $\beta$'s and `$m \times r$' parameters of $\alpha$'s, the criterion in the regression setting is the usual sum of squares,
	\begin{equation} \label{eqn:bigwtheta}
	W(\theta)= \sum_{l=1}^n \sum_{k=1}^m \big(y_{lk} - f_k(x_l)\big)^2 = \sum_{i=1}^m W_i(\theta)
	\end{equation}
and in the classification setting the criterion is, 
	\begin{equation} \label{eqn:bigwtheta2}
	W(\theta)= - \sum_{i=1}^n \sum_{k=1}^m y_{ik} \ln f_k(x_i),
	\end{equation}
which is also termed a measure of deviance. The method to estimate the unknown parameters, `$\theta$' is a gradient descent, also-called as back-propagation method. This is essentially based on the first differential of $W_i(\theta)$; the `$r \times 1$' iteration is
	\begin{equation} \label{eqn:rby1it}
	\hat{\theta}^{(r+1)}= \hat{\theta}^{(r)} - \gamma_r \sum_{i=1}^n \dfrac{\delta W_i(\theta)}{\delta \theta},
	\end{equation}
where $\gamma_r$ is called the learning rate. Details on the choice of the learning rate are discussed in Hastie et al. (2009)~\cite{hastibf}. 


A more general form of neural net is given in Figure~\ref{fig:neural_net} below. The number of layers is to be determined conceptually or empirically, and because these models are over-parametrized, the optimization in \eqref{eqn:rby1it} can be a nonconvex function. As a result, some practical considerations are worth keeping in mind when fitting these models, such as the non-exhaustive examples given below. 


	\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{chapters/chapter_advanced/figures/input_output.png}
	\caption{Feed-forward neural nets. \label{fig:neural_net}}
	\end{figure}


\begin{enumerate}[--]
\item All inputs are to be standardized, as the scaling of these inputs determines the scaling of the weights in the bottom layers.
\item Starting values of `$\theta$' are suggested to be random values near zero, as large values tend to result in inferior solutions.
\item Neural Network models tend to overfit the data and so early stopping rates in the iterative process of \eqref{eqn:rby1it} are suggested. To avoid overfitting, consider minimizing 
	\begin{equation}\label{eqn:overfit}
	W^*(\theta)= W(\theta) + \lambda \theta' \theta,
	\end{equation}
where `$\lambda$' is a positive tuning parameter.
\item Many hidden units ($Z$'s) are preferred as they can capture various non-linearities. 
\item Multiple minima in \eqref{eqn:rby1it} are possible. A suggested approach to avoid this is a method known as `bagging' which calls for computing averages of predictions obtained from random perturbation of training data. \\
\end{enumerate}


% Reinforcement Learning
\subsection{Reinforcement Learning\label{sec:rein_learning}}

The set of techniques covered in this area is quite broad and generally refers to learning from interaction to achieve a goal.\label{in:re_learning} The learner is called an `agent' who learns from interacting with its `environment' through trial and error. The learning is assumed to progress through `rewards.' More formally, the environment is in a state, $s_t \in S$, the set of all possible states; the agent's decision or action, $a_t \in A(s_t)$, the set of actions available in state `$s_t$.' As a consequence, the agent gets a reward $r_{t+1} \in R$ and a new state of the environment, `$s_{t+1}$' emerges. Reinforcement learning methods study how the agent changes the policy, $\Pi_t (s_t,a_t)$ with a goal towards maximizing the reward, a result of the interaction in the long run. For an excellent survey on this topic, refer to Kaelbling, Littman and Moore (1996)~\cite{littmanmoor}. The common assumption is that the state-space is stationary which means that the transition probabilities from one state to another remains constant and so are reward signals. The long-run reward function discounted by learning rate that is optimized by the agent is denoted as, $E\big( \sum_{t=0}^\infty \gamma^t r_t \big)$. Other reward functions such as regret, a measure of the expected decrease in reward gained due to executing an algorithm have been considered in the literature. For a comprehensive treatment of the subject, see Sulton and Barto (2018)~\cite{sultonbarto}. Much of the notes in this section follow from the references cited here. \twomedskip


\noindent Markov Assumption: We assume the sets $S$, $A$ and $R$ are all finite. We assume that any response at `$t+1$' depends on the state and action at `$t$.' More specifically, 
	\begin{equation} \label{eqn:pstpone}
	P(s_{t+1}=S', r_{t+1}=r'  \;|\; F_t)= P(s_{t+1}=S', r_{t+1}=r'  \;|\; s_t,a_t),
	\end{equation}
where $F_t$ is the set of values of all the prior events until `$t$.' It is useful to note that the key quantities that define the dynamics of the decision process under the assumptions are the transition probabilities, $P_{ss'}= P(s_{t+1}=s' \;|\; s_t=s, a_t=a)$ and the anticipated value of the next reward, $R_{ss'}= \Ex(r_{t+1} \;|\; s_t= s, a_t= a, s_{t+1}= s')$. The reinforcement learning algorithms are based on estimating future value functions of state-action pairs which depend on the agent's policy, $\Pi\, (s,a)$. Formally, the state-value function is,
	\begin{equation} \label{eqn:statevalue}
	V^\Pi(s)= \Ex_\Pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \;|\; s_t=s \right],
	\end{equation}
and the action-value function is,
	\begin{equation} \label{eqn:actionvalue}
	q^\Pi(s,a)= \Ex_\Pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \;|\; s_t=s, a_t=a \right].
	\end{equation}
While the above formulation appears to be quite general, consistency conditions imposed to satisfy some recursive relationships lead to a form for $V^\Pi(s)$ called the Bellman equation:
	\begin{equation} \label{eqn:vprods}
	V^\Pi(s)= \sum_a \Pi (s,a) \; \sum_{s'} P_{ss'} [ R_{ss'} + \gamma V^\Pi(s')].
	\end{equation}
The second part of the above equation indicates how the value that the result from future states are discounted and weighted by their probabilities. The optimal state-value function is denoted as
	\begin{equation} \label{eqn:statevstar}
	V^*(s)= \max_\Pi V^\Pi(s)= \max_a \sum_{s'} P_{ss'}^a(R_{ss'}^a + \gamma V^*(s')).
	\end{equation}

The uniqueness and existence of $V^\Pi$ are possible when $\gamma < 1$; it is guaranteed that eventual termination will result from all states under the policy, $\Pi$. \twomedskip


Similarly, the optimal action-value function can be written as
	\begin{equation} \label{eqn:actionvalueqstar}
	Q^*(s,a)= \max_\Pi Q^\Pi(s,a)= \sum_{s'} P_{ss'}^a( R_{ss'}^a + \gamma \max_{a'} Q^*(s',a') ).
	\end{equation}


Exact solutions are possible only under idealized conditions. A possible approach to solve the above is to use dynamic programming, but in practice the methods to solve the problem tend to be heuristic and computational such as using a neural network (Deep Queue Network---DQN) to approximate the action-value function.  



More generally speaking, Reinforcement Learning algorithms can be trained either using Value-based methods ($Q$-Learning) or Policy-based methods (Policy Gradient), so it's worth mentioning a few words about the differences.


In $Q$-learning, as we just described, for a given state we calculate the $Q$ value for every action in the action space and we pick the action corresponding to the max value. This means that choosing actions depends on the $Q$ value at each decision point: $Q$-Learning attempts to learn the value of being in a given state (s), and taking a specific action (a) there with regards to the stated reward structure (per the Bellman equation, the expected long-term reward for a given action is equal to the immediate reward plus the discounted expected reward from the best future action taken at the following state). 


In Policy-based methods, we directly learn the policy function $\Pi$ without explicitly modeling the value function, resulting in actions being taken without directly calculating $Q(s,a)$. This tends to be closer to the actual objective of learning the optimal policy (e.g. learning to trade optimally) directly from experiences, rather that through intermediary steps which are subject to higher noise and uncertainty. An interesting specificity of financial applications---like trading---in favor of a Policy-based approach is that the true reward for the agent tends to be terminal (e.g. P\&L, Execution Slippage, etc.), but the reward of each individual actions are less obvious, nor do they necessarily matter. For instance, when learning to optimally slice a large trade into a multitude of child orders, what is the value of having posted a top-of-book passive order two hours into the execution compared to having posted one level below the top-of-book? While it's hard to to get to these values, one would want the agent to learn to give a value to the intermediate actions in order to guide it toward the ultimate reward. 


While still a nascent field in finance, Reinforcement Learning is evolving fast and hold promises for trading applications that naturally lend themselves to be modeled as Markov Decision Processes. We encourage the curious reader to explore further the vast literature available on this topic.



% Multiple Indicators and Boosting Methods
\subsection{Multiple Indicators and Boosting Methods\label{sec:multindboostmeth}}

In developing models that capture the main features of the data, it must be noted that there are no unique models. Thus they yield different forecasts. It is well-known in the forecasting literature that a combined forecast generally does perform overall better than the individual forecasts. If there are `$T$' models and thus `$T$' forecasts, $\hat{f}_1,\ldots, \hat{f}_T$, how do we combine them to get a single forecast? Because these are all based on the same data, the estimates are likely to be correlated, resulting in a covariance matrix, $\hat{\Sigma}_f$. Three estimates that are proposed in the literature are described in Table~\ref{tab:estimators}. 

	\begin{table}[!ht]
	\caption{Three proposed estimates}\label{tab:estimators}
	\begin{tabular}{l l r}
	\textbf{Weighted Estimator 1: \hskip 1mm} & $\hat{f}_{w_1}=\hat{f}' \, \hat{\Sigma}_f^{-1} \, \hat{f}$ \\
	\textbf{Weighted Estimator 2: \hskip 1mm} & $\hat{f}_{w_2} = \hat{f}' (\text{diag}\hat{\Sigma}_f)^{-1} \hat{f}$ \\
	\textbf{Simple Average Estimator: \hskip 1mm} & $\sum_{i=1}^T \frac{\hat{f}_i}{T}$ 
	 \end{tabular}
	 \end{table}

\noindent Due to the uncertainty in the estimation of $\Sigma_f$, especially in regime changes that may result in large estimation error, the simple average estimator is sometimes advocated. In the context of modeling the asset prices the elements of $\Sigma_f$ represent how well the methods do in forecasting. In order to use the above weighted estimators we need to keep track of the performance of the individual forecasting methods and in principle, the methods that yield inferior forecasts would get less weight. But a question about the span of their performance, that is, whether to use more recent versus past data, remains open. \twomedskip


\noindent \textbf{Ensemble Learning} \twomedskip


As mentioned,\label{in:bagging}\label{in:boost1} the idea of combining multiple forecasts (classifiers) into a single one is a longstanding goal of many decision-makers. This is especially important when methods that are successful in different regimes are complementary. There are two types of ensemble methods, bagging and boosting that we discuss here. Suppose the goal is to determine if we want to enter the market or not based on the information available at that time. It is somewhat easier to come up with simple rules but is harder to find a rule that has high accuracy. Under boosting, we study how to combine the simple rules uniformly into a `stronger' rule. It is suggested to focus on examples that are hardest to forecast by simple rules and take a weighted majority of these rules to come up with a stronger rule.


To motivate, we will assume that we want to determine whether the price of an asset will go up or down. Under the random walk model, the error in guessing randomly is 0.5. But any rule should result in an error less than 0.5. We begin with a training set, $S=\{ (x_i,y_i), i=1,2, \ldots n\}$ where $x_i$ is the information and $y_i$ is a binary variable that indicates whether to enter the market or not. Assume that there are `$T$' rules, $h_t(x)$ are the classifiers and $D_t$ is the distribution over the training set with the error,
	\begin{equation} \label{eq:errdh}
	\text{err}_D(h)=Pr_{(x,y) \sim D}(h(x) \neq y).
	\end{equation}
The rule $h_t$ is supposed to have error $\text{err}_{D_t}(h_t)$ with respect to `$D_t$'. \twomedskip


\noindent The boosting algorithm works as follows:

        \begin{table*}[!ht]
        \begin{tabular}{r l r}
        \textbf{Initialize:} & $D_t(i)=\frac{1}{n}$ for all $i$  \\
        \textbf{Iterate:} & $D_{t+1}(i)=\frac{D_t(i)}{z_t} \cdot \text{exp}[-\alpha_t y_i h_t(x_i)]$, \\
        & where $\alpha_t=\frac{1}{2}\text{ln} \left( \frac{1-\text{err}_{D_t}(h_t)}{\text{err}_{D_t}(h_t)} \right), z_t=$ normalizing constant \\
        \textbf{Stopping Time:} & Stop when the error on a validation data set does \\
        & not improve \\
        \textbf{Final Rule:} & $\text{Sign}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$  \\
        \end{tabular}
        \end{table*}
 
 
It is possible to overfit with boosting; its performance clearly depends upon how well the two decisions (enter or not enter) are clearly separated based on the past data. The use of kernels can improve local separability. It is also important to optimally extract features, $x$, that can be discriminating between the two options for $y$. The main advantage of ensembles of different rules is that it is not likely that all rules will make the same error. The algorithm described above is known as, ``adaptive boosting algorithm (AdaBoost)'' is by Freund and Schapire (1995, 1996,1997)~\cite{freund1995decision,freund1996experiments,freund1997decision} and it automatically ``adapts'' to the data at hand. If $t^{th}$ classifier $h_t(x)$ is parameterized and the overall predictor takes the form
	\begin{equation} \label{eq:hxsign}
	h(x)=\text{Sign} \left( \sum_{t=1}^T \alpha_t \text{Sign}(w^{(t)}x+b^{(t)}) \right)
	\end{equation}
is a two-layer neural network with `$T$' hidden units. One can see the identical underlying nature of the structure between neural network and boosting algorithm.


The idea of bagging comes from training the rules on multiple data sets that supposedly have similar structure. The multiple data sets are obtained by bootstrap  resampling, meaning the data sets are not likely to be too similar. For data on asset prices, which are represented by an AR(1) model with the coefficient close to one, the asymptotic distribution of the sample estimate of the coefficient is non-normal. So the bootstrap samples are also useful to construct the appropriate sampling variance. The bootstrap samples are created by resampling the innovations or empirical residuals. From the single data set, $S$ we create `$M$' bootstrapped samples, $S_1, \ldots, S_M$ with each set containing `$n$' samples. Then apply the boosting method on all these samples and average them. As noted in the literature, bagging tends to reduce variance and thus it provides an alternative to regularization, see Breiman (1996)~\cite{breiman1996bagging}.\label{in:bagging_end}


The gradient boosting method introduced by Breiman (1999)~\cite{breiman1999prediction} and generalized procedure called Logitboost, by Friedman et al. (2000)~\cite{friedman2000additive} and Friedman (2001)~\cite{friedman2001greedy} further improves on the ADA boost. The method starts by fitting an additive model (ensemble), $\sum_t \alpha_t h_t (x)$ in a forward stage-wise manner. In each stage, a weak learner is introduced to compensate for the shortcomings of existing weak learners. In gradient boosting, gradients are used to identify the shortcomings and in the regression context, these are residuals. Now the training set, $S^*=\{ (x_i, y_i-h(x_i)) , i =1,\ldots,n \}$ is treated like, $S$, with residuals playing the role of the data. The optimization occurs by moving in the opposite direction of the gradient of the function\footnote{Gradient boosting is readily implemented in the open-source XGBoost library.}. \label{in:adv_model_end}



% Exercises
\section{Exercises}

The dataset consists of (CSV) file that contains the entire trading session, including early and late hours from 6:00~a.m. to 8:00~p.m. EST. It contains intraday depth book activity several major tickers from up to 5 major national exchanges: Nasdaq, Direct Edge, NYSE, ARCA, and BATS (exact number of exchanges depends on the historical period). All messages are consolidated into one file ordered by timestamp. This dataset allows to build a full national depth book (super book) at any moment intraday. 


For the analysis of Problems 1--3, we consider the data for CISCO. The details of Level~III data are given in Table~\ref{tab:level3data}. \twomedskip


% Problem
\prob You need to aggregate the data into 5-min intervals before answering the following.
        \begin{enumerate}[(a)]
        \item Let $x_t$ denote the number of trades in the $t$th 5-minute interval. Ignore the time gaps between trading days. Plot the time series and its ACF. Determine if there are intraday period patterns in the series.
        \item Using the last transaction in the $t$th 5-minute interval as the stock price in that interval, plot the time series $y_t$ of 5-minute returns during the period and the corresponding ACF.
        \item Consider the bivariate time series ($x_t$, $y_t$). How does $y_t$ vary with $x_t$? Are there any intraday periodic patterns in ($x_t$, $y_t$)?
        \item Plot the durations of the transaction times for Mondays and Fridays. Is there any difference in the patterns?
        \item Fit GARCH models for the durations and interpret the coefficients.
        \item Is there any particular exchange that offers better price? \twomedskip
        \end{enumerate}


% Problem
\prob Use the transaction data in Problem~1. There are seventy-eight five-minute intervals in a trading day. Let $d_i$ be the average of all log durations for the $i$th 5-minute interval across all trading days. Let $t_j$ be tick time of a trade during the $i$th 5-minute interval, define an adjusted duration as $\Delta t_j^* = \Delta t_j/\exp(d_i)$ where $\Delta t_j = t_j - t_{j-1}$.
        \begin{enumerate}[(a)]
        \item Is there a diurnal pattern in the adjusted duration series?
        \item Build EACD and WACD models for the adjusted duration and compare them. Now aggregate the data over the 5-minute intervals; Let $r_t$ be the return and $V_t$ is the volume of transactions in the $t$th interval.
        \item Is there any relationship between the volume $V_t$ and volatility $r_t^2$ . If there is develop a model that will capture the dependence of volatility on volume. Let $\overline{p}_t$ be the average price in the $t$th 5-minute
        interval. \twomedskip
        \end{enumerate}


% Problem
\prob Use the CISCO execution data only; aggregate at various time intervals: 1~min, 2~min, \dots, 15~min. For these levels record
        \begin{enumerate}[(a)]
        \item Average price, VWAP, realized volatility and aggregate volume.
        \item By relating the returns and volatility estimates based on average price to realized volatility, identify the optimal duration for aggregation that will cancel out the market friction but pick up the true information. (You may want to refer to Bandi and Russell (2006)~\cite{bandi}). \twomedskip
        \end{enumerate}