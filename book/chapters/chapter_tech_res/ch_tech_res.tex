% !TEX root = ../../book.tex
%\hfill
%\par\vspace{\baselineskip}

\chapter{The Research Stack}\label{chap:ch_tech_res}
Designing, testing, calibrating, measuring and improving a suite of algorithmic trading strategy is not for the faint of heart. As we have  seen It's a large scale problem that requires a deep investment and can take years to setup. One of the critical component of the overall stack and one that is often underfunded and thus underdeveloped is the research environment. For an execution business this is most often an afterthought, a setup that is scrounged together from existing piece of the infrastructure they already know the need to operate: historical market data, reference data, and historical transactional data in order to fulfill the TCA needs of the clients and a quantitative team often has to do with what is available. \\

Their needs and requirements take a back seat to the needs of the client even if in the end what the client needs is Algo performance not necessarily providing performance reports on  performance that is sub-optimal due to lack of a decent research environment. The shrewd reader will likely notice some animosity in this tirade. Must come from the years of frustration! Things though are getting better and nowadays quantitative teams a much better funded and resourced, often even with dedicated infrastructure and resources to support and maintain their research environment.\\

In this chapter we'll try to provide some pointers of what a best-in-class research stack would look like. The basic components, approaches and considerations born out two decade+  of experience and the trials and tribulations (mostly tribulations!) of yours truly. While most of the chapter is reserved to the ``standard'' setup of an execution business we'll reserve, where appropriate, a short paragraph to describe the setup most often found in an HFT setup as it is quite different and has some interesting features worth at least mentioning.\\


The authors wish they could dedicate the equivalent of a full book (and maybe one day they will) to this topic. Unfortunately the book is already very long and our Editor would love to see this book published before retirement so we are forced to restrain our desire and provide only a brief overview of the topic.  

\section{Data Infrastructure}
Without data there is no research. Period. I then goes without saying then that the most important component of your research environment must be your data infrastructure. There are two main parts of a typical data infrastructure: Historical Market Data, and Historical Transactional Data. \\

Additionally we  should not forget the lessons from chapter~\ref{chap:ch_trading_fund} thus we should make sure we have access to historical reference data as well. While most of the research and analytics will be intro-day and thus less dependent of historical consistency of reference data it is sometime necessary to look at prices/volumes over multiple days which creates the added complexity of potentially having to adjust the data for corporate actions. This is not a simple thing and rarely reference data platform store all ticker changes history, symbology mapping and the various corporate action multipliers needed to adjust the data.

\subsection{Historical Ticker Plant}
Market data, just as the case was for the production tech stack can be the most challenging and expensive component of a data infrastructure. Even for  the simplest usecase: consolidated Level 1 quote and trade data TAQ data from NYSE \footnote{\url{http://www.nyxdata.com/data-products/daily-taq}} at present time charges \$3000 per month of history It is more than 18Gb per day compressed. Any respectable operation needs at least one to two years so just the simplest setup requires an investments of >\$75,000 and storage and processing capacity for 8Tb of data. A more serious setup would have Level 2 data for the top exchanges with costs and sizes several multiples  of what quoted before. \\
Once collected the data has to be processed, normalized and stored in an infrastructure that allows analysis and processing. While theoretically one could store this data in a Relational Database realistically this never creates a usable infrastructure and historically only few vendor infrastructures have been successful in this space. Notably KDB from Kx System \footnote{\url{https://kx.com/}} has been the dominant player in the space despite the steep cost and the very terse and cryptic q language that is a hard to master and thus a very sought after skill \footnote{Some people will argue that q is the reason for kdb success. The jury is out on that}. In recent years this space has evolved significantly and there are now very powerful open-source contenders like InfluxDB \footnote{\url{https://www.influxdata.com/}} and many others. Others have started exploring distributed systems on top of the Hadoop/Spark ecosystem. KDB continues to maintain it's dominance in large scale trading operations but its reign is no longer as certain it used to be.\\

The best setups also collect and provide, through a similar API, intraday real-time market data. This allows a lot of the analysis tools to work both historically and intraday. Since it's quite hard to capture with 100\% accuracy the data at all times very often the intraday data is discarded overnight and replaced with the vendor provided official one that has been processed and adjusted for the various erroneous trade events etc. 

\paragraph{HFT Setup}

Ultra-low latency HFT trading operations usually capture and store the network packets from the raw production exchange feeds for all major exchanges not once but for each data center. This is to be able to train separately the strategy in each data center and seeing the market data from other data centers with the actual delays as the strategy would see in production. Data is kept in a non processed compressed format without the leverage of any time series database. The size of this data is staggering, in the order of >50g compressed per day per data center.

\subsection{Historical Transactional Data}
This is another complicated and large infrastructure problem. Ideally all OMS order and trading events should be stored in a time series db as they happen and then processed and store in a consistent data model. When we were discussing OMS in chapter \ref{chap:ch_tech} we mentioned that it often leverages a bus based infrastructure. One of the important usecases for that is exactly so that a gateway process can listen to all these events and insert them into the trasactional db. \\

One of the important needs for and execution setup is to to be able to fully reconstruct the life-cycle of an order from it's arrival to the inbound gateway, going through the algo container generating orders fro the SOR container to finally create child orders to be sent to the market via the outbound gateway. Additionally if orders are amended from the clients it is necessary to be able to tie this all together within the order hierarchy. This is quite a subtle and complex data modeling problem making the buildout of a data warehousing solution that allows the researcher easy access is massively important. This is something the authors feel very strongly about since it has caused countless hours of extra work and frustrations grappling through the limitations for their own infrastructures.\\

With regards to data warehousing infrastructures we can make similar considerations around vendors and solutions that we made in the section on market data. While KDB is still a strong contender for these solutions it is not as dominant as in the market data domain. \footnote{This is somewhat surprising since, as we will see, a lot of the model calibration and TCA environments we'll discuss in a future sections require the joining and time alignment of both market data and transactional data (the so called asof and windows joins) that are one of the absolute strengths of KDB}. Due to the  complexity of the data modeling part these approached benefit more from the normalized approaches typical of more standard data warehousing solutions built on relational DBs. 

\paragraph{HFT Setup}
As we have seen in chatper~\ref{chap:ch_tech} HFT and market making setups are mostly built in as a single monolithic co-located process. It is also a relatively simplified setup as trading initiation is completely endogenous to the trading strategy and thus does not have to be tied back to initiating order instructions coming down from the OMS. Typical solutions are  lot more ``low-tech'' that come from processing the raw log files and storing the instructions in compressed csv files.

\section{Calibration infrastructure}
As we have seen in previous chapters a respectable trading execution operation requires a whole slew of models and analytics that have to be created, calibrated, and delivered into the production environment. Creating a solid, nimble and flexible calibration environment that provides a high degree of automation, strong validation and one click productionization is not  simple exercise but one that the authors strongly encourage investing time and resources in. This unless one is OK with countless hours spent each month keeping all these models up-to-date, fixing production breaks, and answer embarrassing question on why one could not trade the recent IPO. 

The specific details of a solution for this is very much dependent on the current SDLC and productionization practices within the specific firm so it would be hard to give more specific advice apart from a few pointers:
\begin{itemize}
\item One frequent complication is that many of these processes are usually being built within the research environment leveraging the research data setup that is often different from the production installation. Ideally the two setup match and once the processes have been created the code, not the data/analytics are pushed into production and the data is generated there for consumption of production applications.
\item Avoid a  process for all calibration but opt for a pipeline of independent hierarchical components with intermediary data available for various subsequent steps in the pipeline. This makes the process much easier to extend and parallelize. 
\item think carefully about segmentation of calibration meaning when one should have per symbols analytics vs. per group. Different analytics might rely on different groupings. Should this complexity be supported by the production system or should one keep this as part of the calibration process but in the end``explode'' it to each individual symbol? There are pros and cons.
\item If possible one should try to be robust around new issues/IPOs meaning one should have a process that has a forward looking list of upcoming IPOs so that the reference data and at least sensible default analytics can be created. This is usually a very hard thing to do and IPOs are a frequent pain-point of this process.
\item Think very carefully around default values. What happens if a symbol is not present? Because of the high reliance on normalization variables a mismatch between defaults and realized stock behavior could lead to strong under-performance of the algorithms.
\item Keep a history of the calibration data so that data can be used for historical simulation
\item There is an over-reliance on using flat files (e.g. csv) to deliver this content. A possible better approach is for the data to be stored into a database with a production tag primary key and the production process (possibly and intermediary process that actually creates flat files for the production environment) generates these files based on the primary key which is controlled by production support. 
\end{itemize}

\section{Simulation Environment}
This is a topic that could easily have at least its own chapter if it wasn't for the already hefty size of this manuscript. So we'll be forced to do a cursory overview of the topic. \\

In chapter~\ref{chap:ch_tech} we discussed how a strategy container is usually developed and how the main strategy loop contains most of the decision points on how the trading is done while other parts of the infrastructure keeps the state of the executing order in synch. But even for very simple strategies how does one test that the logic is correct? The most intuitive approach is to build some form of simulation environment that allows the developer to ensure that all the pieces are working together and the strategy makes the ``right'' decisions based on the current state. 

The most effective way to build a simulation environment is to start with the existing strategy container and to stub out  it's inputs and outputs of so that it can run independently.  The most important layers tor replace :
\begin{itemize}
\item Inbound Order Layer: Replace the injection of parent one or more parent orders reading from a flat file or command line
\item Reference and Static Data Layers: Replaces the access to this data with a csv files
\item Market Data Layer: Replacing the market data real-time subscription with a component that injects historical market data from a flat file.
\item Outbound Layer: Replacing the component with a sub that contains some form of fill model.
\end{itemize}

The most complex layers of the above list are of course the Market Data and the Outbound layers which we'll discuss later but there is a very important element that needs  be considered as part of the overall architecture of a strategy container in order to be able to support effectively a simulation environment. This element is time.

\subsection{Handling Time for Simulation}

When one wants to run a simulation of a whole-day VWAP order it is obvious that she does not want to have to wait many hours to find out the result. She would want to run the simulation faster, ideally as fast as events can be processes (the so called ``bullet time''), so she can see the results, make the necessary changes and repeat. The problem is that there are several components part of the strategy container that leverage actual time (e.g. trading sessions such as opening and closing auctions, order start and end time, etc) and there is usually a liberal use of timers to perform specific tasks such as timeouts (e.g. waiting for acks from the exchange after submitting an order) and delays (e.g. have the strategy wake up every n seconds or decide to sleep for a few seconds after a large block fill). Running the strategy simulation in accelerated time while maintaining realistic behavior requires that the time is set as part fo the simulation and the timers are accelerated in a consistent manner.\\

One approach often used in practice is to create an internal time service component within the infrastructure where time is sources and timers are scheduled that can then be replaced by a simulated version that get's its time from one consistent source which is usually the time-stamps of the historical market data being replayed within the Market Data layer. When time is requested by the strategy or container the latest time-stamp processed is returned, if a timer is scheduled the time service will look at the next event time-stamp and if that happens after the target exipiry of the time event either calls back the component or, as it's more likely, injects the time event in the event loop for processing \footnote{The low level details of the threading model used by strategy containers is well beyond the scope of this book. We hope the reader can still grasp the underlying concepts}.

\subsection{Market Data and Order Submission for Simulation}
As discussed this is where most of the complications lie and where there can be a lot of differentiation between approaches. Some approaches are very simple and only really useful as an form of compliance testing, ensuring that the strategy behaves correctly. Other approaches strive towards a more realistic behavior meant to be used to understand and fine tune the expected behavior from a trading performance standpoint. Let' briefly look at some of the approaches used in practice.

\subsubsection{Data replaying and simple fill model}
Arguably the simplest of approaches. The market data layer is simply replaying level 1 (or level 2 depending on the implementation) data into the system. The outbound layer fills any market order at the price available at the far touch or, if using level 2 data, it could provide the fills in a more realistic fashion using the available liquidity at each price level. For limit orders these are usually kept in a list and filled when either there is a trade at the limit price or the market moves so that now the price is marketable. The pros of this approach is it's simplicity but there are many cons. Market data is never updated by the presence of our orders and thus does not incorporate the impact of our aggressive trades and completely ignores queue priority of our limit orders or any change if we orders are meant to improve the near side (as if our orders where hidden). This makes the behavior of the simulator extremely optimistic and not realistic if one wants to use it to fine tune order placement.

\subsubsection{Book building, matching engine, and small order assumption}
A more complex approach calls for order level data (or at least level 2 data that can then be deconstructed into a simplified order level data) that can be used to reconstruct the order book. The order submission layer can be replaced with a simplified version of a matching engine where historical order-book is recreated via the order data feed. That same component is also used as the market data layers thus the strategy receives the market data created by the matching engine. Now any strategy decision to post or take liquidity is overlayed on top of the historical data and faithfully added to the matching engine and market data feed. This approach provides a more realistic environment as market data now changes because to the strategy actions. Queue positioning is now considered at least in some way and when liquidity is taken so that a level is removed, that effect will be visible at least until a new quote on the far side is inserted in the order book with the next market data tick. \\

While more realistic this approach has the implicit assumption that the strategy actions do not change the state of the market, meaning that its decision do not affect the decision of other market participants and any impact to liquidity is strictly temporary in nature. 

\subsubsection{Market Impact and Relative Pricing}
An further extension of the above approach is to try to incorporate some form of impact and orderbook dynamics while still being connected to historical prices. One approach that has been tried in the past was to convert order book data into price relative data meaning that at the beginning the bid and ask are ste based on historical data but new arrival of orders is converted as relative to the reference price. For example if a sell order at the best ask is entered it will be entered at the current 0 level of the ask queue, regardless of the current price. This would allow for the effect of the strategy decision to have a more lasting effect (if a buying strategy takes the whole level a new 0 level ask order will be added at the new increased price). \\

In order for such an approach to avid devolving into an unrealistic orderbook an additional, model driven, process needs to be introduced that injects orders into the engine in order to slowly drive the orderbook, over a certain ``decay time'', back to the historical setting. an approach like this will require some heuristics around liquidity replenishment but could incorporate some form of short term market impact component to add additional realism in the simulation.

\subsubsection{Model Based Order Book}
Finally we turn to a purely model driven approach. To simulate a realistic order book is a very challenging process and we refer you to the in-depth treatment of modeling LOB dynamics in ~\ref{chap:ch_trade_data_models}. We can also point the interested reader to (Huang,Lehalle, Rosenbaum)(2015)~\ref{hulero}. These model will provide some interesting features and might be practical for usage in HFT strategy simulation, but the authors are somewhat skeptical that the behavior of the models is sufficiently nuanced to allow the research to use it as a tuning mechanism for execution strategy, in particular for liquidity seeking and aggressive strategies.

\subsubsection{Simulation of multiple venue for SOR}
The treatment we have made on simulation can expand to support multiple venues and some of the approaches might be relevant for the more complex case. One would need to incorporate a model for simulating dark liquidity to account for the various dark pools. 
\textnote{Point to Chapter 9 if we add some thoughts around dark pool liquidity models}

\subsubsection{Conclusion on Simulators}
Creating a realistic microstrocutre based simulation environment for execution strategies is extremely hard and probably out of reach of practitioners at this point. This makes the joint simulation of several order books and dark pools necessary to train a Smart Order Router a complete pipe dream. There is still a lot of research to be done in this space and we encourage researches and academics to pursue this topic further. At the purely scheduling level, where order placement is essentially abstracted out and averaged across all liquidity sources, this seems a lot more promising in the short term and with some effort one could create a somewhat realistic environment where to train at least some of the standard execution strategies.

\section{TCA Environment}
Once research and simulations are done there is only one way to really know how well the new algorithm (or the changes to said algorithm) and that is putting the strategy in production, use it for trading, and measure \dots and measure \dots and measure. This is were a well architected Transaction Cost Analysis infrastructure becomes critical and possibly were most researchers will end up spending most of their time. 

The shrewd reader could argue that this section would better fit in~\ref{chap:ch_tech} since it's such an integral part of any large execution operation. We chose instead to make it part of the chapter on research stack for a specific reason. As the authors have seen multiple times, this infrastructure is often built as an independent infrastructure that is not linked with the rest of the research environment apart for maybe the data back-ends. We believe this to be a mistake for multiple reasons:
\begin{itemize}
\item Many of the same analytics created for TCA are needed for general research and bespoke analysis with the difference that the number of analytics and other various parameters are usually much larger and changed more frequently. This implies that researches and up building out their own infrastructure that then diverges from the production one leading to duplication of efforts, inconsistencies in the analytics and heavy reliance on bespoke, on the desk, analysis since the production environment always ends up lagging the research one.
\item The focus of standard independent TCA infrastructure is usually to build standard, one-size-fits-all post-trade reports. This ignores the fact that, more and more, clients have very specific needs and benchmarks and in some cases their own bespoke post trade attribution frameworks.  This fluid and state fits poorly into the standard approaches which tends to be too inflexible leading to meaningful development effort and thus long delays to implement client requested customizations.
\item When evaluating the results of simulations one should ideally use the same post-trade infrastructure used for production trades. This is usually very hard, if not impossible, to do if one does not consider both research and production as one interdependent whole.
\item More often than not the resources dedicated to build the TCA infrastructure are the same as the ones used for the overall data and research environments. This, combined with the tendency to put client deliverables always first, leads to usual under-investment of technical resources to build a flexible research and analytics environment that is the cornerstone of any successful operation.
\end{itemize}

Building a flexible, integrated environment is not an easy task and there are no well trodden solutions but it's a very worthy effort to tackle and it will pay off in spades if done correctly. It is challenging from multiple perspective as the need for full access to multiple years of data  for quantitative researchers and the flexibility and quick time to market of new analytics and benchmarks are often in contrast with the stability and repeatability necessary to manage the day-to-day needs of clients. This leads to trick trade-offs and requires some creative thinking. \\ Additionally any change in analytics and benchmarks often require the backfilling of such analytics backward in time so that they are available for historical reports. When one has to deal with many millions of orders over several years, with analytics often needing tick-by-tick data, backfilling itself can be a tough technical challenge.

Going into the full gory details of such an infrastructure is unfortunately beyond the scope of the brief excursion but we propose a few pointers of things that have worked well in the past as  guide for the reader who finds herself in the position of being involved in building such a solution.
\begin{itemize}
\item Consider building a layered, componentized approach. Most of the analytics are constructed using simpler lower level analytics. 
\item Limit the number of times you pass through quotes and trades as this is by far the slowest part of the process. \footnote{Many of the TCA analytics are based on the prevailing quotes and trades at specific times such as the start and the end of the order. This requires what are called  as-of and window joins. The excellent speed at which kdb can perform these joins is one of the main reason for the platform's success. }
\item
\end{itemize}
