% !TEX root = ../../book.tex
\chapter{The Research Stack\label{chap:ch_tech_res}}

Designing, testing, calibrating, measuring and improving a suite of algorithmic trading strategy is not for the faint of heart. As we have discussed, it is a large scale problem that requires deep investment and can take years to set up. One of the critical components of the overall stack, one that is often underfunded and thus underdeveloped, is the research environment. For an execution business this is usually an afterthought, a setup that is scrounged together from already existing pieces of the infrastructure they know that need to operate a quantitative team often has to do with what is available.: historical market data, reference data, and historical transactional data in order to fulfill the transactions cost analysis (TCA) needs of the clients.


The needs and requirements of a research team take a back seat to the needs of the client even if in the end what the client wants is Algo performance. This may not necessarily result in providing performance reports on  performance that may be sub-optimal due to lack of a decent research environment. The shrewd reader may notice some frustration in this statement but it comes from years of experience. Things though are getting better and quantitative teams are nowadays much better funded and resourced. They even have dedicated infrastructure and resources to support and maintain the research environment.


In this chapter, we attempt to provide some pointers of what a best-in-class research stack would look like. The basic components, approaches and considerations born out two decades of experience and the trials and tribulations (mostly tribulations!). While most of the chapter is reserved to the ``standard'' setup of an execution business we will reserve, where appropriate, a short paragraph to describe the setup most often found in an HFT outfit as it is quite different and has some interesting features at least worth mentioning.


The authors wish they could dedicate a full book (and maybe one day they will) to this topic, as there is so much to be said based on our practical experience. 



% Data Infrastructure
\section{Data Infrastructure\label{sec:tech_data_infra}}\label{in:dat_infr1}

As we have seen throughout this book, without data, obviously, there is no research. It then goes without saying that the most important component of research environment must be data infrastructure. There are two main parts of a typical data infrastructure: Historical Market Data, and Historical Transactional Data. 


Additionally as observed from Chapter~\ref{chap:ch_trading_fund}, we should have access to historical reference data as well. While most of the research and analytics will be intra-day and thus less dependent on historical data, it is sometimes necessary to look at prices/volumes over multiple days for consistency. This creates an added complexity of potentially having to adjust the data for corporate actions. This is not a simple thing and rarely reference data platforms store all history of ticker changes, symbology mapping and various corporate action multipliers that were used to adjust the data. \twomedskip


% Historical Ticker Plant:
\noindent\textbf{Historical Ticker Plant:} Market data, just as the case was for the production tech stack can be the most challenging and expensive component of a data infrastructure. Even for  the simplest usecase: consolidated Level~I quote and trade data TAQ data\label{in:taq3} from NYSE\footnote{\url{http://www.nyxdata.com/data-products/daily-taq}} at present time charges \$3000 per month of history. It is more than 18~GB per day compressed. Any respectable operation needs at least one to two years so just the simplest setup requires an investment of at least \$75,000 and storage and processing capacity for 8~TB of data. A more serious setup would have Level~II data for the top exchanges with costs and sizes, several multiples of Level~I data. \label{in:level2dat2}


Once collected the data has to be processed, normalized and stored in an infrastructure that allows for analysis and for processing. While theoretically one could store this data in a relational database, realistically this never creates a usable infrastructure and historically only few vendor infrastructures have been successful in this space. Notably KDB from Kx System\footnote{\url{https://kx.com/}} has been the dominant player in this space, despite the steep cost and the terse and cryptic `q' language that is a hard to master and thus it is a very much sought-after skill.\footnote{Some people will argue that q is the reason for KDB success. The jury is out on that.} In recent years this space has evolved significantly and there are now powerful open-source contenders like InfluxDB\footnote{\url{https://www.influxdata.com/}} and many others. Others have started exploring distributed systems on top of the Hadoop/Spark ecosystem. KDB continues to maintain its dominance in large scale trading operations but its reign is no longer as certain as it used to be.


The best setups also collect and provide, through a similar API, intraday real-time market data. This allows for an extensive analysis and tools to work both historically and intraday. Since it is quite hard to capture with 100\% accuracy the data at all times, more often the intraday data is discarded overnight and replaced with the vendor provided official one that has been processed and adjusted for various erroneous trade events. \twomedskip



\pagebreak



% HFT Setup
\noindent\emph{HFT Setup} \twomedskip

Ultra-low latency HFT trading operations usually capture and store the network packets from raw production feeds for all major exchanges, not one but for each data center. This is to be able to train separately the strategy in each data center and to see the market data from other data centers with the actual delays as the strategy goes through production. Data is kept in a non-processed compressed format without the leverage of any time series database. The size of this data can be staggering, in the order of more than 50~GB compressed per day per data center. \twomedskip


% Historical Transactional Data:
\noindent\textbf{Historical Transactional Data:} This is another complicated and large infrastructure problem. Ideally all OMS order and trading events should be stored in a time series database as they happen and then processed and stored in a consistent data structure. As in Chapter~\ref{chap:ch_tech}, we mentioned the OMS often leverages a bus-based infrastructure. One of the important usecases for that to happen is to have a gateway process that can listen to all these events and insert them into the transactional database.


An important need for any execution setup is to to be able to fully reconstruct the life-cycle of an order from its arrival to the inbound gateway, passing through the algorithm container generating orders for the SOR container to finally create child orders, to be sent to the market via the outbound gateway. Additionally if orders get amended by the clients it is necessary to be able to tie all this together within the order hierarchy. This is quite a subtle and complex data modeling problem. The buildout of a data warehousing solution that allows the researcher easy access is massively important. This is something the authors feel very strongly about since it may cause countless hours of extra work and frustrations, grappling through the limitations for their own infrastructures.


With regards to data warehousing infrastructures we can make similar considerations around vendors and solutions that we made in the section on market data. While KDB is still a strong contender for these solutions it is not as dominant as in the market data domain.\footnote{This is somewhat surprising since, as we will see, a lot of the model calibration and TCA environments we will discuss in a future sections require the joining and time alignment of both market data and transactional data (the so-called asof and windows joins) that are one of the absolute strengths of KDB.} Due to the complexity of the data modeling part, these approaches benefit more from the normalized approaches, typical of more standard data warehousing solutions built on relational databases. 


As we have seen in Chapter~\ref{chap:ch_tech}, HFT and market-making setups are mostly built in as a single monolithic co-located processes. It is also a relatively simplified setup as trading initiation is completely endogenous to the trading strategy and thus does not have to be tied back to initiating order instructions coming down from the OMS. Typical solutions are more ``low-tech'' that come from processing raw log files and storing the instructions in compressed csv formatted files.\label{in:dat_infr2}



% Calibration infrastructure
\section{Calibration Infrastructure\label{sec:calb_infra}} \label{in:calb1}

From the discussion presented here, it is clear that a respectable trading execution operation requires a whole slew of models and analytics that have to be created, calibrated, and delivered into the production environment. Creating a solid, nimble and flexible calibration environment that provides a high degree of automation, has strong validation capabilities and one click productionization is not a simple exercise but one that we strongly encourage investing time and resources in. This, unless one is fine with countless hours spent each month keeping all these models up-to-date, fixing production breaks and answering embarrassing questions such as why one could not trade the recent IPO. 


The specific details of a solution for this is very much dependent on the current SDLC and productionization practices within a specific firm. So it would be hard to give more specific advice apart from a few general pointers:

\begin{itemize}
\item A frequent complication is that many of these processes are usually built within the research environment leveraging the research data setup that is often different from the production installation setup. Ideally when the two setups match and the processes have been created, the code, not the data/analytics are pushed into production. The data is generated there for consumption of production applications.

\item It is better to avoid a calibration of an entire process but opt for a pipeline of independent hierarchical components with intermediary data, available for various subsequent steps in the pipeline. This makes the process much easier to extend and parallelize. 

\item One would think carefully about segmentation of calibration, meaning when one should have per symbol analytics vs. per group. Different analytics might rely on different groupings. Should this complexity be supported by the production system or should one keep this as part of the calibration process but in the end can ``explode'' it to each individual symbol is not an easy one to answer. There are pros and cons.

\item If possible, it is better to be robust around new issues/IPOs meaning that there should be a process in place that is forward looking and has a list of upcoming IPOs so that the reference data and at minimum sensible default analytics can be created. This is usually a very hard thing to do and IPOs are a frequent pain-point of this process.

\item It is essential to think very carefully around default values. What happens if a symbol is not present? Because of the high reliance on normalization variables a mismatch between defaults and realized stock behavior could lead to strong under-performance of the algorithms.

\item Maintain a history of the calibration data so that data can be used for historical simulation.

\item There is generally an over-reliance on using flat files (e.g. csv) to deliver this content. A possible better approach is for the data to be stored into a database with a production tag primary key and the production process (possibly an intermediary process that actually creates flat files for the production environment) that generates these files based on the primary key which is controlled by production support. \label{in:calb2}
\end{itemize}



% Simulation Environment
\section{Simulation Environment\label{sec:sim_env}}

This is a topic that deserves an extensive discussion but here we will provide only a cursory overview. In Chapter~\ref{chap:ch_tech}, we discussed how a strategy container is usually developed and how the main strategy loop contains most of the decision points on how the trading is done while other parts of the infrastructure keep the state of the executing order in sync. But even for very simple strategies how do we test that the logic is correct? The most intuitive approach is to build a simulation environment that allows the developer to ensure that all the pieces are working together and the strategy makes the ``right'' decisions based on the current state. 


The most effective way to build a simulation environment is to start with the existing strategy container and to stub out its inputs and outputs of so that it can run independently.  The most important layers to replace are:
        \begin{itemize}
        \item Inbound Order Layer: Replace the injection of one or more parent orders reading from a flat file or command line.
        \item Reference and Static Data Layers: Replace the access to this data with a csv files.
        \item Market Data Layer: Replace the market data, the real-time subscription with a component that injects historical market data from a flat file.
        \item Outbound Layer: Replace the component with a substitute that contains some form of fill model.
        \end{itemize}


The most complex layers of the above list are of course the Market Data and the Outbound layers which we will discuss later but there is a very important element that needs be considered as part of the overall architecture of a strategy container in order to be able to support effectively a simulation environment. This element is in real time. \twomedskip


% Handling Time for Simulation:
\noindent\textbf{Handling Time for Simulation:} When simulation of a whole-day VWAP order is desired, it is obvious that we may not want to have to wait many hours to find out the result. The simulation should execute faster, ideally as fast as events can be processed (the so-called ``bullet time''), so the results can be quickly reviewed. The necessary changes can be made and repeated. The problem is that there are several components, part of the strategy container that leverage actual time, e.g. trading sessions such as opening and closing auctions, order start and end time, etc., and there is usually a liberal use of timers to perform specific tasks such as timeouts, e.g. waiting for acknowledgement from the exchange after submitting an order, and delays, e.g. have the strategy wake up every `$n$' seconds or decide to sleep for a few seconds after a large block fill. Running the strategy simulation in accelerated time while maintaining realistic behavior requires that the time is set as part fo the simulation process and the timers are accelerated in a consistent manner.


One approach often used in practice is to create an internal time service component within the infrastructure. Time is sourced and timers are scheduled that can then be replaced by a simulated version. It gets its time from one consistent source which is usually the time-stamps of the historical market data, being replayed and replicated within the Market Data layer. When time is requested by the strategy or container, the latest time-stamp processed is returned. If a timer is scheduled, the time service will look at the next event time-stamp and if that happens after the target expiry, the time event either calls back the component or, as it is more likely, will inject the time event in the event loop for processing.\footnote{The low level details of the threading model used by strategy containers is well beyond the scope of this book. We hope the reader can still grasp the underlying concepts.} \twomedskip


% Market Data and Order Submission for Simulation:
\noindent\textbf{Market Data and Order Submission for Simulation:} As discussed, this is where most of the complications lie and where there can be a great deal of differentiation between approaches. Some approaches are very simple and are only really useful as a form of compliance testing, ensuring that the strategy behaves correctly. Other approaches strive towards a more realistic behavior meant to be used to understand and fine-tune the expected behavior from a trading performance standpoint. We briefly look at some of the approaches used in practice. \twomedskip


% Data Replaying and Simple Fill Model:
\noindent\textbf{Data Replaying and Simple Fill Model:} Arguably, this is the simplest of approaches. The market data layer will simply replay Level~I (or Level~II depending on the implementation) data into the system. The outbound layer fills any market order at the price available at the far touch or, if using Level~II data, it could provide the fills in a more realistic fashion using the available liquidity at each price level. For limit orders these are usually kept in a list and filled when either there is a trade at the limit price or the market moves toward the posted price so that now the price is marketable. The advantage of this approach is its simplicity but there are many disadvantages. Market data is not updated by the presence of our orders and thus does not incorporate the impact of any aggressive trades and completely ignores queue priority of the limit orders or any change, if the orders are meant to improve the near side (as if our orders were hidden). This makes the behavior of the simulator extremely optimistic and is not realistic if one wants to use it to fine-tune the order placement. \twomedskip


% Book Building, Matching Engine, and Small Order Assumption:
\noindent\textbf{Book Building, Matching Engine, and Small Order Assumption:} A more complex approach calls for order level data (or at least Level~II data that can then be deconstructed into a simplified order level data) that can be used to reconstruct the order book. The order submission layer can be replaced with a simplified version of a matching engine where historical order-book is recreated via the order data feed. The same component is also used as the market data layers. Thus, the strategy receives the market data created by the matching engine. Now any strategy decision, to post or take liquidity, is overlaid on top of the historical data and is faithfully added to the matching engine and market data feed. This approach provides a more realistic environment as market data now changes because of the strategy actions. Queue positioning is now considered in some way and when liquidity is taken so that a level is removed, that effect will be visible at least until a new quote on the far side is inserted in the order book when the next market data tick happens.


While more realistic, this approach has the implicit assumption that the strategy actions do not change the state of the market, meaning that its decision does not affect the decision of other market participants and any impact on liquidity is strictly temporary in nature. \twomedskip


% Market Impact and Relative Pricing:
\noindent\textbf{Market Impact and Relative Pricing:} An extension of the above approach is to try to incorporate some form of impact and order book dynamics, while still being connected to historical prices. One approach that has been tried in the past was to convert order book data into price relative data. That is, at the beginning, the bid and ask prices are stated based on historical data but new arrival of orders is converted as relative to the reference price. For example if a sell order at the best ask is entered it will be entered at the current (zero) level of the ask queue, regardless of the current price. This would allow for the effect of the strategy decision to have a more lasting effect (if a buying strategy takes away the whole level a new zero level ask order will be added at the new increased price). 


In order for such an approach to avoid devolving into an unrealistic orderbook an additional, model driven, process is introduced. This injects orders into the engine in order to slowly drive the orderbook, over a certain ``decay time'', back to the historical setting. An approach like this will require some heuristics around liquidity replenishment but could incorporate some form of short term market impact component to add additional realism in the simulation. \twomedskip


% Model Based Order Book:
\noindent\textbf{Model Based Order Book:} Finally, we turn to a purely model driven approach. To simulate a realistic order book is a challenging process and we refer you to the in-depth treatment of modeling LOB dynamics as discussed in Chapter~\ref{chap:ch_trade_data_models}. We can also point the interested reader to Huang, Lehalle, Rosenbaum (2015)~\cite{hlehros}. These models provide some interesting features and might be of practical usage in HFT strategy simulation. But the authors are somewhat skeptical if the behavior of the models is sufficiently nuanced to allow the research to use it as a tuning mechanism for execution strategy, in particular for liquidity seeking and aggressive strategies.
\twomedskip


% Simulation of Multiple Venue for SOR:
\noindent\textbf{Simulation of Multiple Venue for SOR:} The treatment we have made on simulation can expand to support multiple venues and some of the approaches might be relevant for the more complex scenarios. One would need to incorporate a model for simulating dark liquidity to account for the trading in various dark pools. Additionally one would have to incorporate the facts that all these venues are connected by regulation and latency arbitrage to be effective in realistically replicating the complex interaction across the liquidity spectrum.\twomedskip


% Conclusion on Simulators:
\noindent\textbf{Conclusion on Simulators:} Creating a realistic microstructure based simulation environment for execution strategies is extremely hard and probably out of reach of practitioners at this point. This makes the joint simulation of several order books and dark pools necessary to train a Smart Order Router a completely unchartered area. There is still more research to be done in this space and we encourage researchers, academics and fintechs to pursue this topic further. At the purely scheduling level, where order placement is essentially abstracted out and averaged across all liquidity sources, this seems a lot more promising in the short term. With some effort, one could create a somewhat more realistic environment on where to train some standard execution strategies.



% TCA Environment
\section{TCA Environment\label{sec:tca_envir}}

Once research and simulations are done there is only one way to really know how well the new algorithm works: put the strategy in production, use it for trading, and then measure\dots and measure\dots and measure its performance. This is a well architected Transaction Cost Analysis (TCA) infrastructure which becomes critical and possibly where most researchers will end up spending their time. 


The shrewd reader could argue that this section would better fit in Chapter~\ref{chap:ch_tech} since it is such an integral part of any large execution operation. We chose instead to make it part of this chapter on research stack for a specific reason. As the authors have seen multiple times, this infrastructure is often built as an independent infrastructure that is not linked with rest of the research environment apart for maybe the data back-ends. We believe this to be a mistake for multiple reasons:

\begin{itemize}
\item Many of the same analytics created for TCA are needed for general research as well, and bespoke analysis with the difference that the number of analytics and other various parameters are usually much larger and change more frequently. This implies that researchers end up building out their own infrastructure. This then diverges from the production leading to duplication of efforts, inconsistencies in the analytics and heavy reliance on bespoke, on the desk. Analysis of changes since the production environment always ends up lagging the research one.

\item The focus of standard independent TCA infrastructure is usually to build standard, one-size-fits-all post-trade reports. This ignores the fact that, more and more, clients have specific needs and benchmarks and in some cases their own bespoke post trade attribution frameworks. This fluid state fits poorly into the standard approaches which tend to be too inflexible for any meaningful development effort and thus results in long delays to implement client requested customizations.

\item When evaluating the results of simulations we should ideally use the same post-trade infrastructure that is used for production trades. This is usually very hard, if not impossible, to do if one does not consider both research and production as one interdependent entity.

\item More often than not the resources dedicated to build the TCA infrastructure are the same as the ones used for the overall data and research environments. This, combined with the tendency to put client deliverables always first, leads to usual under-investment of technical resources to build a flexible research and analytics environment which is the cornerstone of any successful operation.
\end{itemize}


Building a flexible, integrated environment is not an easy task. There are no well trodden solutions but it is a worthy effort to tackle and it pays off in spades if done correctly. It is challenging from many perspectives as the need for full access to multiple years of data  for quantitative researchers and the flexibility and quick time to market of new analytics and benchmarks are often in contrast with the need for stability and repeatability necessary to manage the day-to-day client demands. This leads to difficult trade-offs and requires some creative thinking. Additionally any change in analytics and benchmarks often require the backfilling of such analytics past in time so that they are available for historical reports. When we deal with many millions of orders over several years, with analytics often needing tick-by-tick data, backfilling itself can be a tough technical challenge.


Going into the full cumbersome details of such an infrastructure is unfortunately beyond the scope of the brief excursion. But we propose a few pointers that have worked well in the past as a guide for the reader who may be involved in building such a solution.

\begin{itemize}
\item Consider building a layered, componentized approach. Layered because most of the analytics are constructed using simpler lower level analytics. `Componentized' implies subdividing the TCA process in logical components that can then be assembled into simple daily reports or full fledged quarterly review report. An approach that considers levering a micro-services middleware is growing in popularity in recent years.\footnote{\url{https://en.wikipedia.org/wiki/Microservices}}

\item Limit the number of times you pass through quotes and trades as this step is by far the slowest part of the process.\footnote{Many of the TCA analytics are based on the prevailing quotes and trades at specific times such as the start and the end of the order. This requires what are called  as-of and window joins. The excellent speed at which KDB can perform these joins is one of the main reason for the platform's success.}

\item The ideal setup, rarely achieved in practice, should be able to generate most of the $T+0$ analytics on the fly as the algorithms complete. This would allow for the post-trade data to be available at the end of the trading day and thus reports can be generated and sent to clients immediately. This is something that clients really appreciate. This requires a strongly-integrated infrastructure that leverages all the same components at runtime as it does after the fact. 

\item Consider making the TCA development infrastructure as an extension of your research environment that is under continuous integration. This means that building and running regression tests every time something is changed. This will allow for quick turn-around in changes and immediate promotion into production. Leverage the same enrichment and post-trade analytics and apply them to the output of any simulation that is run for a specific scenario and store the results separately for each scenario. This will simplify the analysis and comparison of different parametrization and functionality.

\item Instead of backfilling all new analytics for all clients/time-frame, one may consider implementation forward demand backfilling. This means that each component that is requested and for which the analytics are unavailable will compute the analytics for the client/period requested. This same approach can be used overnight to trigger the backfill  (or re-populate in case it had to be fixed) of analytics to accelerate the delivery of large scale request.

\item Unless necessary, avoid the temptation into investing into an infrastructure for a fully fledged custom report builder. Instead, focus on making customized report that is easy to programmatically create and assemble with all branding and formatting already in place. More and more people in sales and coverage have (or should have) at least some basic programming skills that will enable them to build additional custom reports. Some of these reports can then be standardized for clients who may not have complex needs. Report builders' framework is in general a significant development effort, complicated and error prone to build. They are also usually inflexible and are hard to extend. 
\end{itemize}

 

% Conclusion
\section{Conclusion\label{sec:tech_concl}}

It has been the authors' experience that rarely enough time is ever spent in building a powerful and flexible research environment that provides quantitative researchers the tools to quickly access and analyze data, create supporting analytics, simulate behavior of algorithms and then to be able to analyze the behavior of the algorithms and generate post trade analytics for their own and their clients' review. Done correctly though, this should become a real engine for innovation and performance and a real competitive advantage in a highly competitive field. We hope this brief but broad review will be useful to help the reader who is involved in such a buildout, the tools needed to point in the right direction.\label{in:exec_tech2}