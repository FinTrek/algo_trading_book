% !TEX root = ../../book.tex
%\hfill
%\par\vspace{\baselineskip}

\chapter{The Research Stack}\label{chap:ch_tech_res}
Designing, testing, calibrating, measuring and improving a suite of algorithmic trading strategy is not for the faint of heart. As we have  seen It's a large scale problem that requires a deep investment and can take years to setup. One of the critical component of the overall stack and one that is often underfunded and thus underdeveloped is the research environment. For an execution business this is most often an afterthought, a setup that is scrounged together from existing piece of the infrastructure they already know the need to operate: historical market data, reference data, and historical transactional data in order to fulfill the TCA needs of the clients and a quantitative team often has to do with what is available. \\

Their needs and requirements take a back seat to the needs of the client even if in the end what the client needs is Algo performance not necessarily providing performance reports on  performance that is sub-optimal due to lack of a decent research environment. The shrewd reader will likely notice some animosity in this tirade. Must come from the years of frustration! Things though are getting better and nowadays quantitative teams a much better funded and resourced, often even with dedicated infrastructure and resources to support and maintain their research environment.\\

In this chapter we'll try to provide some pointers of what a best-in-class research stack would look like. The basic components, approaches and considerations born out two decade+  of experience and the trials and tribulations (mostly tribulations!) of yours truly. While most of the chapter is reserved to the ``standard" setup of an execution business we'll reserve, where appropriate, a short paragraph to describe the setup most often found in an HFT setup as it is quite different and has some interesting features worth at least mentioning.\\


The authors wish they could dedicate the equivalent of a full book (and maybe one day they will) to this topic. Unfortunately the book is already very long and our Editor would love to see this book published before retirement so we are forced to restrain our desire and provide only a brief overview of the topic.  

\section{Data Infrastructure}
Without data there is no research. Period. I then goes without saying then that the most important component of your research environment must be your data infrastructure. There are two main parts of a typical data infrastructure: Historical Market Data, and Historical Transactional Data. \\

Additionally we  should not forget the lessons from chapter~\ref{chap:ch_trading_fund} thus we should make sure we have access to historical reference data as well. While most of the research and analytics will be intro-day and thus less dependent of historical consistency of reference data it is sometime necessary to look at prices/volumes over multiple days which creates the added complexity of potentially having to adjust the data for corporate actions. This is not a simple thing and rarely reference data platform store all ticker changes history, symbology mapping and the various corporate action multipliers needed to adjust the data.

\subsection{Historical Ticker Plant}
Market data, just as the case was for the production tech stack can be the most challenging and expensive component of a data infrastructure. Even for  the simplest usecase: consolidated Level 1 quote and trade data TAQ data from NYSE \footnote{\url{http://www.nyxdata.com/data-products/daily-taq}} at present time charges \$3000 per month of history It is more than 18Gb per day compressed. Any respectable operation needs at least one to two years so just the simplest setup requires an investments of >\$75,000 and storage and processing capacity for 8Tb of data. A more serious setup would have Level 2 data for the top exchanges with costs and sizes several multiples  of what quoted before. \\
Once collected the data has to be processed, normalized and stored in an infrastructure that allows analysis and processing. While theoretically one could store this data in a Relational Database realistically this never creates a usable infrastructure and historically only few vendor infrastructures have been successful in this space. Notably KDB from Kx System \footnote{\url{https://kx.com/}} has been the dominant player in the space despite the steep cost and the very terse and cryptic q language that is a hard to master and thus a very sought after skill \footnote{Some people will argue that q is the reason for kdb success. The jury is out on that}. In recent years this space has evolved significantly and there are now very powerful open-source contenders like InfluxDB \footnote{\url{https://www.influxdata.com/}} and many others. Others have started exploring distributed systems on top of the Hadoop/Spark ecosystem. KDB continues to maintain it's dominance in large scale trading operations but its reign is no longer as certain it used to be.\\

The best setups also collect and provide, through a similar API, intraday real-time market data. This allows a lot of the analysis tools to work both historically and intraday. Since it's quite hard to capture with 100\% accuracy the data at all times very often the intraday data is discarded overnight and replaced with the vendor provided official one that has been processed and adjusted for the various erroneous trade events etc. 

\paragraph{HFT Setup}

Ultra-low latency HFT trading operations usually capture and store the network packets from the raw production exchange feeds for all major exchanges not once but for each data center. This is to be able to train separately the strategy in each data center and seeing the market data from other data centers with the actual delays as the strategy would see in production. Data is kept in a non processed compressed format without the leverage of any time series database. The size of this data is staggering, in the order of >50g compressed per day per data center.

\subsection{Historical Transactional Data}
This is another complicated and large infrastructure problem. Ideally all OMS order and trading events should be stored in a time series db as they happen and then processed and store in a consistent data model. When we were discussing OMS in chapter \ref{chap:ch_tech} we mentioned that it often leverages a bus based infrastructure. One of the important usecases for that is exactly so that a gateway process can listen to all these events and insert them into the trasactional db. \\

One of the important needs for and execution setup is to to be able to fully reconstruct the life-cycle of an order from it's arrival to the inbound gateway, going through the algo container generating orders fro the SOR container to finally create child orders to be sent to the market via the outbound gateway. Additionally if orders are amended from the clients it is necessary to be able to tie this all together within the order hierarchy. This is quite a subtle and complex data modeling problem making the buildout of a data warehousing solution that allows the researcher easy access is massively important. This is something the authors feel very strongly about since it has caused countless hours of extra work and frustrations grappling through the limitations for their own infrastructures.\\

With regards to data warehousing infrastructures we can make similar considerations around vendors and solutions that we made in the section on market data. While KDB is still a strong contender for these solutions it is not as dominant as in the market data domain. \footnote{This is somewhat surprising since, as we will see, a lot of the model calibration and TCA environments we'll discuss in a future sections require the joining and time alignment of both market data and transactional data (the so called asof and windows joins) that are one of the absolute strengths of KDB}. Due to the  complexity of the data modeling part these approached benefit more from the normalized approaches typical of more standard data warehousing solutions built on relational DBs. 

\paragraph{HFT Setup}
As we have seen in chatper~\ref{chap:ch_tech} HFT and market making setups are mostly built in as a single monolithic co-located process. It is also a relatively simplified setup as trading initiation is completely endogenous to the trading strategy and thus does not have to be tied back to initiating order instructions coming down from the OMS. Typical solutions are  lot more ``low-tech" that come from processing the raw log files and storing the instructions in compressed csv files.

\section{Calibration infrastructure}
As we have seen in previous chapters a respectable trading execution operation requires a whole slew of models and analytics that have to be created, calibrated, and delivered into the production environment. Creating a solid, nimble and flexible calibration environment that provides a high degree of automation, strong validation and one click productionization is not  simple exercise but one that the authors strongly encourage investing time and resources in. This unless one is OK with countless hours spent each month keeping all these models up-to-date, fixing production breaks, and answer embarrassing question on why one could not trade the recent IPO. 

The specific details of a solution for this is very much dependent on the current SDLC and productionization practices within the specific firm so it would be hard to give more specific advice apart from a few pointers:
\begin{itemize}
\item One frequent complication is that many of these processes are usually being built within the research environment leveraging the research data setup that is often different from the production installation. Ideally the two setup match and once the processes have been created the code, not the data/analytics are pushed into production and the data is generated there for consumption of production applications.
\item Avoid a  process for all calibration but opt for a pipeline of independent hierarchical components with intermediary data available for various subsequent steps in the pipeline. This makes the process much easier to extend and parallelize. 
\item think carefully about segmentation of calibration meaning when one should have per symbols analytics vs per group. Different analytics might rely on different groupings. Does this complexity be supported by the production system or should one keep this as part of the calibration process but in the end``explode" it to each individual symbol? There are pros and cons.
\item If possible one should try to be robust around new issues/IPOs meaning one should have a process that has a forward looking list of upcoming IPOs so that the reference data and at least sensible default analytics can be created. This is usually a very hard thing to do and IPOs are a frequent pain-point of this process.
\item Think very carefully around default values. What happens if a symbol is not present? Because of the high reliance on normalization variables a mismatch between defaults and realized stock behavior could lead to strong under-performance of the algorithms.
\item Keep a history of the calibration data so that data can be used for historical simulation
\item There is an over-reliance on using flat files (e.g. csv) to deliver this content. A possible better approach is for the data to be stored into a database with a production tag primary key and the production process (possibly and intermediary process that actually creates flat files for the production environment) generates these files based on the primary key which is controlled by production support. 
\end{itemize}

\section{Simulation Environment}
This is a topic that could easily have at least its own chapter if it wasn't for the already hefty size of this manuscript. So we'll be forced to do a cursory overview of the topic. \\

In chapter~\ref{chap:ch_tech} we discussed how a strategy container is usually developed and how the main strategy loop contains most of the decision points on how the trading is done while other parts of the infrastructure keeps the state of the executing order in synch. But even for very simple strategies how does one test that the logic is correct? The most intuitive approach is to build some form of simulation environment that allows the developer to ensure that all the pieces are working together and the strategy makes the ``right'' decisions based on the current state. 

The most effective way to build a simulation environment is to start with the existing strategy container and to stub out  it's inputs and outputs of so that it can run independently.  The most important layers tor replace :
\begin{itemize}
\item Inbound Order Layer: Replace the injection of parent one or more parent orders reading from a flat file or command line
\item Reference and Static Data Layers: Replaces the access to this data with a csv files
\item Market Data Layer: Replacing the market data real-time subscription with a component that injects historical market data from a flat file.
\item Outbound Layer: Replacing the component with a sub that contains some form of fill model.
\end{itemize}

The most complex layers of the above list are of course the Market Data and the Outbound layers which we'll discuss later but there is a very important element that needs  be considered as part of the overall architecture of a strategy container in order to be able to support effectively a simulation environment. This element is time.

When one wants to run a simulation of a whole day VWAP execution it is obvious that one does not want to have to wait many hours to find out the result. One would want to run the simulation as quickly as possible so she can see the results, make the necessary changes and repeat. The problem is that there are several components part of the strategy container that leverage actual time (e.g. trading sessions such as opening and closing auctions, order start and end time, etc) and there is a healthy use of timers to perform specific tasks such as timeouts (e.g. waiting for acks from the exchange after submitting an order) and delays (e.g. have the strategy wake up every n seconds or decide to sleep for a few seconds after a large block fill). Running the strategy simulation in accelerated time while maintaining realistic behavior requires that the time and the timers 

\section{TCA Environment}

